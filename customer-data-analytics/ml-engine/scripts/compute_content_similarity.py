#!/usr/bin/env python3
"""
Content-Based Item Similarity Computation
Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ content-based Ð¿Ð¾Ñ…Ð¾Ð¶ÐµÑÑ‚Ð¸ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²

Author: Customer Data Analytics Team
"""

import os
import sys
import numpy as np
import pandas as pd
import psycopg2
import json
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from scipy.sparse import csr_matrix, hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
import warnings
warnings.filterwarnings('ignore')

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('content_similarity.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°
SIMILARITY_CONFIG = {
    'weights': {
        'tfidf_tags': 0.4,
        'brand': 0.2,
        'category': 0.1,
        'style': 0.1,  # Ð±ÑƒÐ´ÐµÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ðº ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¼
        'numeric': 0.3
    },
    'tfidf': {
        'max_features': 500,  # Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ
        'min_df': 2,         # Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 Ñ‚Ð¾Ð²Ð°Ñ€Ð° Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¸Ð¼ÐµÑ‚ÑŒ Ñ‚ÐµÐ³
        'max_df': 0.8,       # Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ñ‡Ð°ÑÑ‚Ñ‹Ðµ
        'ngram_range': (1, 1)
    },
    'numeric_features': ['price_current', 'popularity_30d', 'rating'],
    'top_k': 50,             # Ñ‚Ð¾Ð¿-50 Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ñ… Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°
    'min_sim_score': 0.01    # Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð°
}

class ContentSimilarityComputer:
    def __init__(self, db_connection_string: str):
        self.db_conn_str = db_connection_string
        self.conn = None
        self.products_df = None
        self.feature_vectors = None
        self.similarity_matrix = None
        
    def connect_db(self):
        """ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð‘Ð”"""
        try:
            self.conn = psycopg2.connect(self.db_conn_str)
            logger.info("âœ… Connected to database")
        except Exception as e:
            logger.error(f"âŒ Database connection failed: {e}")
            raise
    
    def load_product_features(self) -> pd.DataFrame:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð¸Ð· Ð‘Ð”"""
        query = """
        SELECT 
            product_id,
            brand,
            category,
            color,
            size,
            material,
            gender,
            style,
            price_current,
            rating,
            popularity_30d,
            tags_normalized,
            tags_count,
            title,
            description_short,
            is_active
        FROM ml_item_content_features
        WHERE is_active = true
        ORDER BY product_id
        """
        
        logger.info("ðŸ“Š Loading product features from database...")
        self.products_df = pd.read_sql(query, self.conn)
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
        self.products_df['brand'] = self.products_df['brand'].fillna('unknown')
        self.products_df['category'] = self.products_df['category'].fillna('unknown')
        self.products_df['style'] = self.products_df['style'].fillna('unknown')
        self.products_df['rating'] = self.products_df['rating'].fillna(2.5)
        self.products_df['popularity_30d'] = self.products_df['popularity_30d'].fillna(0)
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐ³Ð¾Ð²
        self.products_df['tags_text'] = self.products_df['tags_normalized'].apply(
            lambda x: ' '.join(x) if x and isinstance(x, list) else ''
        )
        
        logger.info(f"ðŸ“ˆ Loaded {len(self.products_df)} active products")
        return self.products_df
    
    def create_tfidf_features(self) -> csr_matrix:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ TF-IDF Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ Ñ‚ÐµÐ³Ð¾Ð²"""
        logger.info("ðŸ”¤ Creating TF-IDF vectors for tags...")
        
        vectorizer = TfidfVectorizer(
            max_features=SIMILARITY_CONFIG['tfidf']['max_features'],
            min_df=SIMILARITY_CONFIG['tfidf']['min_df'],
            max_df=SIMILARITY_CONFIG['tfidf']['max_df'],
            ngram_range=SIMILARITY_CONFIG['tfidf']['ngram_range'],
            token_pattern=r'\b[a-zA-Z]+\b',  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð±ÑƒÐºÐ²ÐµÐ½Ð½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹
            lowercase=True,
            stop_words=None
        )
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ TF-IDF Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ
        tfidf_matrix = vectorizer.fit_transform(self.products_df['tags_text'])
        
        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        feature_names = vectorizer.get_feature_names_out()
        logger.info(f"ðŸ“Š TF-IDF vocabulary size: {len(feature_names)}")
        logger.info(f"ðŸ“Š TF-IDF matrix shape: {tfidf_matrix.shape}")
        logger.info(f"ðŸ“Š TF-IDF sparsity: {(1 - tfidf_matrix.nnz / tfidf_matrix.size) * 100:.1f}%")
        
        # Ð¢Ð¾Ð¿-20 ÑÐ°Ð¼Ñ‹Ñ… Ð²Ð°Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÐ³Ð¾Ð²
        feature_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()
        top_features_idx = np.argsort(feature_scores)[-20:]
        top_features = [feature_names[i] for i in top_features_idx[::-1]]
        logger.info(f"ðŸ·ï¸ Top tags: {', '.join(top_features[:10])}")
        
        return tfidf_matrix, vectorizer
    
    def create_categorical_features(self) -> csr_matrix:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ one-hot Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²"""
        logger.info("ðŸ“‚ Creating categorical features...")
        
        categorical_matrices = []
        
        # Brand features
        brand_encoder = LabelEncoder()
        brand_encoded = brand_encoder.fit_transform(self.products_df['brand'])
        brand_matrix = self._create_onehot_matrix(brand_encoded, len(brand_encoder.classes_))
        categorical_matrices.append(brand_matrix)
        logger.info(f"ðŸ·ï¸ Brand features: {len(brand_encoder.classes_)} unique brands")
        
        # Category features  
        category_encoder = LabelEncoder()
        category_encoded = category_encoder.fit_transform(self.products_df['category'])
        category_matrix = self._create_onehot_matrix(category_encoded, len(category_encoder.classes_))
        categorical_matrices.append(category_matrix)
        logger.info(f"ðŸ“‚ Category features: {len(category_encoder.classes_)} unique categories")
        
        # Style features
        style_encoder = LabelEncoder()
        style_encoded = style_encoder.fit_transform(self.products_df['style'])
        style_matrix = self._create_onehot_matrix(style_encoded, len(style_encoder.classes_))
        categorical_matrices.append(style_matrix)
        logger.info(f"ðŸŽ¨ Style features: {len(style_encoder.classes_)} unique styles")
        
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        categorical_features = hstack(categorical_matrices)
        logger.info(f"ðŸ“Š Total categorical features shape: {categorical_features.shape}")
        
        return categorical_features
    
    def _create_onehot_matrix(self, encoded_values: np.ndarray, n_classes: int) -> csr_matrix:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ one-hot Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹"""
        from scipy.sparse import csr_matrix
        rows = np.arange(len(encoded_values))
        cols = encoded_values
        data = np.ones(len(encoded_values))
        return csr_matrix((data, (rows, cols)), shape=(len(encoded_values), n_classes))
    
    def create_numeric_features(self) -> np.ndarray:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²"""
        logger.info("ðŸ”¢ Creating numeric features...")
        
        numeric_cols = SIMILARITY_CONFIG['numeric_features']
        numeric_data = self.products_df[numeric_cols].copy()
        
        # Ð›Ð¾Ð³Ð°Ñ€Ð¸Ñ„Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ñ†ÐµÐ½Ñ‹ Ð¸ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ð¾ÑÑ‚Ð¸
        numeric_data['price_current'] = np.log1p(numeric_data['price_current'])
        numeric_data['popularity_30d'] = np.log1p(numeric_data['popularity_30d'])
        
        # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ
        scaler = StandardScaler()
        numeric_features = scaler.fit_transform(numeric_data)
        
        logger.info(f"ðŸ“Š Numeric features shape: {numeric_features.shape}")
        logger.info(f"ðŸ“Š Feature means: {numeric_features.mean(axis=0)}")
        logger.info(f"ðŸ“Š Feature stds: {numeric_features.std(axis=0)}")
        
        return numeric_features
    
    def combine_features(self) -> Tuple[csr_matrix, Dict]:
        """ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ñ‚Ð¸Ð¿Ð¾Ð² Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ñ Ð²ÐµÑÐ°Ð¼Ð¸"""
        logger.info("ðŸ”— Combining all feature types...")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
        tfidf_matrix, tfidf_vectorizer = self.create_tfidf_features()
        categorical_matrix = self.create_categorical_features()
        numeric_matrix = self.create_numeric_features()
        
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð²ÐµÑÐ°
        weights = SIMILARITY_CONFIG['weights']
        
        # Ð’Ð·Ð²ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼ TF-IDF
        tfidf_weighted = tfidf_matrix * weights['tfidf_tags']
        
        # Ð’Ð·Ð²ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ (Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð²ÐµÑÐ°)
        cat_weight_per_feature = (weights['brand'] + weights['category'] + weights['style']) / categorical_matrix.shape[1]
        categorical_weighted = categorical_matrix * cat_weight_per_feature
        
        # Ð’Ð·Ð²ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ
        numeric_weight_per_feature = weights['numeric'] / numeric_matrix.shape[1]
        numeric_weighted = numeric_matrix * numeric_weight_per_feature
        
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð² Ð¾Ð´Ð½Ñƒ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ
        from scipy.sparse import csr_matrix
        numeric_sparse = csr_matrix(numeric_weighted)
        
        combined_features = hstack([tfidf_weighted, categorical_weighted, numeric_sparse])
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        feature_info = {
            'tfidf_features': tfidf_matrix.shape[1],
            'categorical_features': categorical_matrix.shape[1], 
            'numeric_features': numeric_matrix.shape[1],
            'total_features': combined_features.shape[1],
            'vectorizer': tfidf_vectorizer
        }
        
        logger.info(f"ðŸŽ¯ Combined features shape: {combined_features.shape}")
        logger.info(f"ðŸ“Š Feature breakdown: TF-IDF={feature_info['tfidf_features']}, "
                   f"Categorical={feature_info['categorical_features']}, "
                   f"Numeric={feature_info['numeric_features']}")
        
        self.feature_vectors = combined_features
        return combined_features, feature_info
    
    def compute_similarity_matrix(self, features: csr_matrix) -> np.ndarray:
        """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ cosine similarity Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹"""
        logger.info("ðŸ”„ Computing cosine similarity matrix...")
        
        n_items = features.shape[0]
        logger.info(f"ðŸ“Š Computing similarity for {n_items} items...")
        
        # Ð”Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð°Ñ‚Ñ€Ð¸Ñ† Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð°Ñ‚Ñ‡Ð¸
        batch_size = 100
        similarity_matrix = np.zeros((n_items, n_items), dtype=np.float32)
        
        for i in range(0, n_items, batch_size):
            end_i = min(i + batch_size, n_items)
            batch_features = features[i:end_i]
            
            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ð¼Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð° (ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°)
            for j in range(i, n_items, batch_size):
                end_j = min(j + batch_size, n_items)
                other_features = features[j:end_j]
                
                # Cosine similarity
                sim_batch = cosine_similarity(batch_features, other_features)
                similarity_matrix[i:end_i, j:end_j] = sim_batch
                
                # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡Ð½ÑƒÑŽ Ñ‡Ð°ÑÑ‚ÑŒ
                if i != j:
                    similarity_matrix[j:end_j, i:end_i] = sim_batch.T
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"â³ Processed {i + batch_size}/{n_items} items...")
        
        logger.info("âœ… Similarity matrix computation completed")
        self.similarity_matrix = similarity_matrix
        return similarity_matrix
    
    def extract_top_similar(self, similarity_matrix: np.ndarray) -> List[Dict]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð¿-K Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ñ… Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾"""
        logger.info(f"ðŸ” Extracting top-{SIMILARITY_CONFIG['top_k']} similar items...")
        
        similarities = []
        n_items = similarity_matrix.shape[0]
        
        for i in range(n_items):
            product_id = self.products_df.iloc[i]['product_id']
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°
            sim_scores = similarity_matrix[i]
            
            # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÑÐµÐ±Ñ Ð¸ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ
            sim_scores[i] = -1  # Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÑÐ°Ð¼Ð¾Ð³Ð¾ ÑÐµÐ±Ñ
            top_indices = np.argsort(sim_scores)[::-1]
            
            # Ð‘ÐµÑ€ÐµÐ¼ Ñ‚Ð¾Ð¿-K Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¿Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ Ð¿Ð¾Ñ€Ð¾Ð³Ñƒ
            count = 0
            for idx in top_indices:
                if count >= SIMILARITY_CONFIG['top_k']:
                    break
                
                sim_score = sim_scores[idx]
                if sim_score < SIMILARITY_CONFIG['min_sim_score']:
                    continue
                
                similar_product_id = self.products_df.iloc[idx]['product_id']
                
                similarities.append({
                    'product_id': int(product_id),
                    'similar_product_id': int(similar_product_id),
                    'sim_score': float(sim_score),
                    'features_used': 'tfidf_tags,brand,category,style,numeric'
                })
                count += 1
        
        logger.info(f"ðŸ“Š Generated {len(similarities)} similarity pairs")
        return similarities
    
    def save_similarities_to_db(self, similarities: List[Dict]):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² Ð‘Ð”"""
        logger.info("ðŸ’¾ Saving similarities to database...")
        
        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        with self.conn.cursor() as cur:
            cur.execute("DELETE FROM ml_item_sim_content")
            logger.info("ðŸ—‘ï¸ Cleared old similarity data")
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ batch insert
        insert_data = []
        for sim in similarities:
            insert_data.append((
                sim['product_id'],
                sim['similar_product_id'], 
                sim['sim_score'],
                sim['features_used'],
                json.dumps({})  # Ð¿Ð¾ÐºÐ° Ð¿ÑƒÑÑ‚Ð¾Ð¹ breakdown
            ))
        
        # Batch insert
        insert_query = """
        INSERT INTO ml_item_sim_content 
        (product_id, similar_product_id, sim_score, features_used, sim_breakdown)
        VALUES (%s, %s, %s, %s, %s)
        """
        
        with self.conn.cursor() as cur:
            cur.executemany(insert_query, insert_data)
            self.conn.commit()
        
        logger.info(f"âœ… Saved {len(similarities)} similarity records to database")
    
    def run_content_similarity_pipeline(self):
        """Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ content-based ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð°"""
        logger.info("ðŸš€ Starting content-based similarity computation pipeline...")
        start_time = datetime.now()
        
        try:
            # 1. ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð‘Ð” Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
            self.connect_db()
            self.load_product_features()
            
            # 2. Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
            features, feature_info = self.combine_features()
            
            # 3. Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð°
            similarity_matrix = self.compute_similarity_matrix(features)
            
            # 4. Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð¿-K
            similarities = self.extract_top_similar(similarity_matrix)
            
            # 5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð‘Ð”
            self.save_similarities_to_db(similarities)
            
            # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("ðŸŽ‰ Content similarity pipeline completed successfully!")
            logger.info(f"â±ï¸ Total duration: {duration}")
            logger.info(f"ðŸ“Š Products processed: {len(self.products_df)}")
            logger.info(f"ðŸ“Š Similarity pairs generated: {len(similarities)}")
            logger.info(f"ðŸ“Š Average similarities per product: {len(similarities) / len(self.products_df):.1f}")
            
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()


def main():
    """Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ"""
    # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº Ð‘Ð”
    db_connection = "postgresql://mikitavalkunovich@localhost:5432/customer_data"
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð°
    computer = ContentSimilarityComputer(db_connection)
    computer.run_content_similarity_pipeline()


if __name__ == "__main__":
    main()
