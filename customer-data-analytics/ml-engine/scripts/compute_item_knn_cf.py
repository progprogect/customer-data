#!/usr/bin/env python3
"""
Item-kNN Collaborative Filtering Computation
–í—ã—á–∏—Å–ª–µ–Ω–∏–µ item-item CF –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ-–ø–æ–∫—É–ø–æ–∫

Author: Customer Data Analytics Team
"""

import os
import sys
import numpy as np
import pandas as pd
import psycopg2
import psycopg2.extras
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from scipy.sparse import csr_matrix, coo_matrix
from sklearn.metrics.pairwise import cosine_similarity
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('item_knn_cf.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
CF_CONFIG = {
    'min_co_users': 5,           # –º–∏–Ω–∏–º—É–º –æ–±—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    'min_item_purchases': 5,     # –º–∏–Ω–∏–º—É–º –ø–æ–∫—É–ø–æ–∫ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —É—á–∞—Å—Ç–∏—è –≤ CF
    'top_k': 50,                 # —Ç–æ–ø-50 –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤
    'min_sim_score': 0.01,       # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
    'similarity_metric': 'cosine' # cosine –∏–ª–∏ jaccard
}

class ItemKNNCollaborativeFiltering:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Item-kNN Collaborative Filtering"""
    
    def __init__(self, db_connection_string: str):
        self.db_conn_str = db_connection_string
        self.conn = None
        self.interactions_df = None
        self.user_item_matrix = None
        self.item_similarities = None
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        try:
            self.conn = psycopg2.connect(self.db_conn_str)
            logger.info("‚úÖ Connected to database")
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {e}")
            raise
    
    def load_interactions(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-—Ç–æ–≤–∞—Ä"""
        logger.info("üìä Loading user-item interactions...")
        
        query = """
        SELECT 
            user_id,
            product_id,
            event_ts,
            amount
        FROM ml_interactions_implicit
        WHERE event_ts >= NOW() - INTERVAL '6 months'
        ORDER BY user_id, product_id, event_ts
        """
        
        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
            cur.execute(query)
            interactions = pd.DataFrame(cur.fetchall())
        
        if interactions.empty:
            raise ValueError("No interactions found")
        
        logger.info(f"üìà Loaded {len(interactions)} interactions")
        logger.info(f"üë• Users: {interactions['user_id'].nunique()}")
        logger.info(f"üõçÔ∏è Products: {interactions['product_id'].nunique()}")
        
        self.interactions_df = interactions
        return interactions
    
    def filter_items_by_popularity(self, interactions: pd.DataFrame) -> pd.DataFrame:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ–∫—É–ø–æ–∫"""
        logger.info(f"üîç Filtering items with min {CF_CONFIG['min_item_purchases']} purchases...")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        item_user_counts = interactions.groupby('product_id')['user_id'].nunique()
        popular_items = item_user_counts[
            item_user_counts >= CF_CONFIG['min_item_purchases']
        ].index
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        filtered_interactions = interactions[
            interactions['product_id'].isin(popular_items)
        ]
        
        logger.info(f"üìä Kept {len(popular_items)} items out of {len(item_user_counts)}")
        logger.info(f"üìä Filtered interactions: {len(filtered_interactions)}")
        
        return filtered_interactions
    
    def create_user_item_matrix(self, interactions: pd.DataFrame) -> Tuple[csr_matrix, Dict, Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π user√óitem –º–∞—Ç—Ä–∏—Ü—ã"""
        logger.info("üî¢ Creating user√óitem matrix...")
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤
        unique_users = interactions['user_id'].unique()
        unique_items = interactions['product_id'].unique()
        
        user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
        item_to_idx = {item_id: idx for idx, item_id in enumerate(unique_items)}
        idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}
        idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (–±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞: 1 = –ø–æ–∫—É–ø–∞–ª, 0 = –Ω–µ –ø–æ–∫—É–ø–∞–ª)
        user_item_agg = interactions.groupby(['user_id', 'product_id']).size().reset_index(name='count')
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è sparse –º–∞—Ç—Ä–∏—Ü—ã
        rows = [user_to_idx[user_id] for user_id in user_item_agg['user_id']]
        cols = [item_to_idx[item_id] for item_id in user_item_agg['product_id']]
        data = [1] * len(rows)  # –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        
        # –°–æ–∑–¥–∞–µ–º sparse –º–∞—Ç—Ä–∏—Ü—É
        matrix_shape = (len(unique_users), len(unique_items))
        user_item_matrix = csr_matrix((data, (rows, cols)), shape=matrix_shape)
        
        logger.info(f"üìä Matrix shape: {matrix_shape}")
        logger.info(f"üìä Matrix density: {user_item_matrix.nnz / (matrix_shape[0] * matrix_shape[1]) * 100:.3f}%")
        logger.info(f"üìä Non-zero entries: {user_item_matrix.nnz:,}")
        
        mappings = {
            'user_to_idx': user_to_idx,
            'item_to_idx': item_to_idx,
            'idx_to_user': idx_to_user,
            'idx_to_item': idx_to_item
        }
        
        self.user_item_matrix = user_item_matrix
        return user_item_matrix, mappings, {'users': len(unique_users), 'items': len(unique_items)}
    
    def compute_item_similarities(self, user_item_matrix: csr_matrix, mappings: Dict) -> List[Dict]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ cosine similarity –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏"""
        logger.info("üîÑ Computing item-item cosine similarities...")
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è item√óuser
        item_user_matrix = user_item_matrix.T
        
        n_items = item_user_matrix.shape[0]
        logger.info(f"üìä Computing similarities for {n_items} items...")
        
        # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        batch_size = 100
        all_similarities = []
        
        for i in range(0, n_items, batch_size):
            end_i = min(i + batch_size, n_items)
            batch_matrix = item_user_matrix[i:end_i]
            
            # Cosine similarity —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏
            similarities = cosine_similarity(batch_matrix, item_user_matrix)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–≤–∞—Ä –≤ –±–∞—Ç—á–µ
            for batch_idx, global_idx in enumerate(range(i, end_i)):
                item_id = mappings['idx_to_item'][global_idx]
                item_similarities = similarities[batch_idx]
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –ø–æ—Ö–æ–∂–∏—Ö (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º —Ç–æ–≤–∞—Ä)
                item_similarities[global_idx] = -1  # –∏—Å–∫–ª—é—á–∞–µ–º —Å–µ–±—è
                top_indices = np.argsort(item_similarities)[::-1]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥–∞–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                count = 0
                for similar_idx in top_indices:
                    if count >= CF_CONFIG['top_k']:
                        break
                    
                    sim_score = item_similarities[similar_idx]
                    if sim_score < CF_CONFIG['min_sim_score']:
                        continue
                    
                    similar_item_id = mappings['idx_to_item'][similar_idx]
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                    item_users = set(user_item_matrix[:, global_idx].nonzero()[0])
                    similar_users = set(user_item_matrix[:, similar_idx].nonzero()[0])
                    co_users = len(item_users & similar_users)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å–æ-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                    if co_users < CF_CONFIG['min_co_users']:
                        continue
                    
                    all_similarities.append({
                        'product_id': int(item_id),
                        'similar_product_id': int(similar_item_id),
                        'sim_score': float(sim_score),
                        'co_users': int(co_users)
                    })
                    count += 1
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"‚è≥ Processed {end_i}/{n_items} items...")
        
        logger.info(f"‚úÖ Generated {len(all_similarities)} similarity pairs")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if all_similarities:
            sim_scores = [s['sim_score'] for s in all_similarities]
            co_users_counts = [s['co_users'] for s in all_similarities]
            
            logger.info(f"üìä Similarity stats:")
            logger.info(f"   Avg similarity: {np.mean(sim_scores):.3f}")
            logger.info(f"   Min similarity: {np.min(sim_scores):.3f}")
            logger.info(f"   Max similarity: {np.max(sim_scores):.3f}")
            logger.info(f"   Avg co-users: {np.mean(co_users_counts):.1f}")
            logger.info(f"   Min co-users: {np.min(co_users_counts)}")
            logger.info(f"   Max co-users: {np.max(co_users_counts)}")
        
        self.item_similarities = all_similarities
        return all_similarities
    
    def save_similarities_to_db(self, similarities: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î"""
        logger.info("üíæ Saving CF similarities to database...")
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        with self.conn.cursor() as cur:
            cur.execute("DELETE FROM ml_item_sim_cf WHERE algorithm = %s", 
                       (CF_CONFIG['similarity_metric'],))
            logger.info("üóëÔ∏è Cleared old CF similarity data")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è batch insert
        insert_data = []
        for sim in similarities:
            insert_data.append((
                sim['product_id'],
                sim['similar_product_id'], 
                sim['sim_score'],
                sim['co_users'],
                f"item_knn_{CF_CONFIG['similarity_metric']}"
            ))
        
        # Batch insert
        insert_query = """
        INSERT INTO ml_item_sim_cf 
        (product_id, similar_product_id, sim_score, co_users, algorithm)
        VALUES (%s, %s, %s, %s, %s)
        """
        
        with self.conn.cursor() as cur:
            cur.executemany(insert_query, insert_data)
            self.conn.commit()
        
        logger.info(f"‚úÖ Saved {len(similarities)} CF similarity records to database")
    
    def validate_cf_quality(self) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ CF —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        logger.info("üîç Validating CF similarity quality...")
        
        validation_query = """
        SELECT 
            COUNT(DISTINCT product_id) as products_with_similarities,
            COUNT(*) as total_similarity_pairs,
            AVG(sim_score) as avg_similarity,
            MIN(sim_score) as min_similarity,
            MAX(sim_score) as max_similarity,
            AVG(co_users) as avg_co_users,
            MIN(co_users) as min_co_users,
            MAX(co_users) as max_co_users
        FROM ml_item_sim_cf
        WHERE algorithm = %s
        """
        
        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
            cur.execute(validation_query, (f"item_knn_{CF_CONFIG['similarity_metric']}",))
            stats = cur.fetchone()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–æ–≤–∞—Ä–æ–≤
        coverage_query = """
        WITH cf_products AS (
            SELECT DISTINCT product_id 
            FROM ml_item_sim_cf 
            WHERE algorithm = %s
        ),
        active_products AS (
            SELECT DISTINCT product_id 
            FROM ml_item_content_features 
            WHERE is_active = true
        )
        SELECT 
            COUNT(cf.product_id) as cf_products,
            COUNT(ap.product_id) as active_products,
            COUNT(cf.product_id) * 100.0 / COUNT(ap.product_id) as coverage_percent
        FROM active_products ap
        LEFT JOIN cf_products cf ON ap.product_id = cf.product_id
        """
        
        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
            cur.execute(coverage_query, (f"item_knn_{CF_CONFIG['similarity_metric']}",))
            coverage = cur.fetchone()
        
        results = {
            'products_with_similarities': stats['products_with_similarities'],
            'total_similarity_pairs': stats['total_similarity_pairs'],
            'avg_similarity': float(stats['avg_similarity']),
            'similarity_range': [float(stats['min_similarity']), float(stats['max_similarity'])],
            'avg_co_users': float(stats['avg_co_users']),
            'co_users_range': [int(stats['min_co_users']), int(stats['max_co_users'])],
            'coverage_percent': float(coverage['coverage_percent']),
            'algorithm': f"item_knn_{CF_CONFIG['similarity_metric']}",
            'config': CF_CONFIG
        }
        
        logger.info("üìä CF Quality Report:")
        logger.info(f"   Products with similarities: {results['products_with_similarities']}")
        logger.info(f"   Coverage: {results['coverage_percent']:.1f}%")
        logger.info(f"   Avg similarity: {results['avg_similarity']:.3f}")
        logger.info(f"   Avg co-users: {results['avg_co_users']:.1f}")
        
        return results
    
    def run_item_knn_cf_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ Item-kNN CF"""
        logger.info("üöÄ Starting Item-kNN Collaborative Filtering pipeline...")
        start_time = datetime.now()
        
        try:
            # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.connect_db()
            interactions = self.load_interactions()
            
            # 2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            filtered_interactions = self.filter_items_by_popularity(interactions)
            
            # 3. –°–æ–∑–¥–∞–Ω–∏–µ user√óitem –º–∞—Ç—Ä–∏—Ü—ã
            user_item_matrix, mappings, matrix_info = self.create_user_item_matrix(filtered_interactions)
            
            # 4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ item-item similarities
            similarities = self.compute_item_similarities(user_item_matrix, mappings)
            
            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            if similarities:
                self.save_similarities_to_db(similarities)
                
                # 6. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                quality_report = self.validate_cf_quality()
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                end_time = datetime.now()
                duration = end_time - start_time
                
                logger.info("üéâ Item-kNN CF pipeline completed successfully!")
                logger.info(f"‚è±Ô∏è Total duration: {duration}")
                logger.info(f"üìä Processed {matrix_info['items']} items, {matrix_info['users']} users")
                logger.info(f"üìä Generated {len(similarities)} similarity pairs")
                
                return quality_report
            else:
                raise ValueError("No similarities generated - check thresholds")
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    db_connection = "postgresql://mikitavalkunovich@localhost:5432/customer_data"
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º CF –∫–æ–º–ø—å—é—Ç–µ—Ä
    cf_computer = ItemKNNCollaborativeFiltering(db_connection)
    quality_report = cf_computer.run_item_knn_cf_pipeline()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"item_knn_cf_quality_report_{timestamp}.json"
    
    with open(report_file, 'w') as f:
        json.dump(quality_report, f, indent=2)
    
    logger.info(f"üíæ Quality report saved to {report_file}")


if __name__ == "__main__":
    main()
