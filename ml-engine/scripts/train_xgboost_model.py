#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XGBoost Model Training Pipeline
–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è purchase_next_30d

Features:
- Precision-focused optimization
- Feature importance analysis
- SHAP explanations
- Production-ready model serialization

Author: Customer Data Analytics Team
"""

import pandas as pd
import numpy as np
import joblib
import json
import warnings
warnings.filterwarnings('ignore')

# ML libraries
import xgboost as xgb
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, average_precision_score,
    roc_curve, precision_score, recall_score, f1_score
)
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
import shap

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# System libraries
import logging
import sys
import os
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('xgboost_training.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class XGBoostTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ XGBoost –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, precision_focused=True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        
        Args:
            precision_focused: –ï—Å–ª–∏ True, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ–¥ precision
        """
        self.precision_focused = precision_focused
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.training_history = {}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º feature columns (–∏—Å–∫–ª—é—á–∞–µ–º target –∏ meta)
        self.feature_columns = [
            'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
            'orders_lifetime', 'revenue_lifetime', 'categories_unique'
        ]
        
        logger.info("üöÄ XGBoost Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        if precision_focused:
            logger.info("üéØ –†–µ–∂–∏–º: PRECISION-FOCUSED (–º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è false positives)")
    
    def load_data(self) -> tuple:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–ª–∏—Ç–æ–≤
            train_df = pd.read_csv('train_set.csv')
            valid_df = pd.read_csv('valid_set.csv')
            test_df = pd.read_csv('test_set.csv')
            
            logger.info(f"‚úÖ Train: {len(train_df):,} —Å—Ç—Ä–æ–∫")
            logger.info(f"‚úÖ Valid: {len(valid_df):,} —Å—Ç—Ä–æ–∫")
            logger.info(f"‚úÖ Test: {len(test_df):,} —Å—Ç—Ä–æ–∫")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö —Ñ–∏—á–µ–π
            missing_features = set(self.feature_columns) - set(train_df.columns)
            if missing_features:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing_features}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π –∏ —Ç–∞—Ä–≥–µ—Ç–∞
            X_train = train_df[self.feature_columns].copy()
            y_train = train_df['target'].copy()
            
            X_valid = valid_df[self.feature_columns].copy()
            y_valid = valid_df['target'].copy()
            
            X_test = test_df[self.feature_columns].copy()
            y_test = test_df['target'].copy()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
            train_nan = X_train.isnull().sum().sum()
            valid_nan = X_valid.isnull().sum().sum()
            test_nan = X_test.isnull().sum().sum()
            
            if train_nan + valid_nan + test_nan > 0:
                logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã NaN: train={train_nan}, valid={valid_nan}, test={test_nan}")
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ train
                for col in self.feature_columns:
                    median_val = X_train[col].median()
                    X_train[col].fillna(median_val, inplace=True)
                    X_valid[col].fillna(median_val, inplace=True)
                    X_test[col].fillna(median_val, inplace=True)
                logger.info("‚úÖ NaN –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–∞ —Ñ–∏—á–µ–π
            self.feature_names = self.feature_columns.copy()
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Å–æ–≤
            train_pos_rate = y_train.mean()
            valid_pos_rate = y_valid.mean()
            test_pos_rate = y_test.mean()
            
            logger.info(f"üéØ Class balance:")
            logger.info(f"   Train: {train_pos_rate:.1%} positive")
            logger.info(f"   Valid: {valid_pos_rate:.1%} positive")
            logger.info(f"   Test:  {test_pos_rate:.1%} positive")
            
            self.training_history['data_stats'] = {
                'train_size': len(train_df),
                'valid_size': len(valid_df),
                'test_size': len(test_df),
                'train_pos_rate': float(train_pos_rate),
                'valid_pos_rate': float(valid_pos_rate),
                'test_pos_rate': float(test_pos_rate),
                'features_count': len(self.feature_columns),
                'feature_names': self.feature_columns
            }
            
            return X_train, X_valid, X_test, y_train, y_valid, y_test
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def fit_scaler(self, X_train: pd.DataFrame) -> pd.DataFrame:
        """–û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞"""
        logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ StandardScaler...")
        
        self.scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ —Å–∫–µ–π–ª–∏–Ω–≥–∞
        logger.info(f"‚úÖ –°–∫–µ–π–ª–∏–Ω–≥ –ø—Ä–∏–º–µ–Ω–µ–Ω. –°—Ä–µ–¥–Ω–∏–µ: {X_train_scaled.mean().round(3).to_dict()}")
        
        return X_train_scaled
    
    def transform_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Å–∫–µ–π–ª–µ—Ä–∞"""
        if self.scaler is None:
            raise ValueError("–°–∫–µ–π–ª–µ—Ä –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ fit_scaler() —Å–Ω–∞—á–∞–ª–∞.")
        
        X_scaled = pd.DataFrame(
            self.scaler.transform(X),
            columns=X.columns,
            index=X.index
        )
        return X_scaled
    
    def calculate_scale_pos_weight(self, y_train: pd.Series) -> float:
        """–†–∞—Å—á–µ—Ç scale_pos_weight –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
        neg_count = (y_train == 0).sum()
        pos_count = (y_train == 1).sum()
        scale_pos_weight = neg_count / pos_count
        
        logger.info(f"üìä Class distribution: {neg_count:,} negative, {pos_count:,} positive")
        logger.info(f"‚öñÔ∏è Calculated scale_pos_weight: {scale_pos_weight:.2f}")
        
        return scale_pos_weight
    
    def get_hyperparameter_space(self, scale_pos_weight: float) -> dict:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        if self.precision_focused:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ precision
            param_space = {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [3, 4, 5, 6],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'min_child_weight': [1, 3, 5, 7],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5, 1.0],
                'reg_lambda': [1, 1.5, 2.0],
                'scale_pos_weight': [scale_pos_weight * 0.8, scale_pos_weight, scale_pos_weight * 1.2]
            }
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            param_space = {
                'n_estimators': [100, 200, 300],
                'max_depth': [4, 5, 6],
                'learning_rate': [0.05, 0.1, 0.2],
                'min_child_weight': [1, 3, 5],
                'subsample': [0.8, 0.9],
                'colsample_bytree': [0.8, 0.9],
                'scale_pos_weight': [scale_pos_weight]
            }
        
        logger.info(f"üé≤ Hyperparameter space size: {np.prod([len(v) for v in param_space.values()]):,} combinations")
        return param_space
    
    def precision_scorer(self, estimator, X, y):
        """–ö–∞—Å—Ç–æ–º–Ω—ã–π scorer –¥–ª—è precision"""
        y_pred = estimator.predict(X)
        return precision_score(y, y_pred, zero_division=0)
    
    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, 
                   X_valid: pd.DataFrame, y_valid: pd.Series) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å hyperparameter tuning"""
        logger.info("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–∏...")
        
        # –†–∞—Å—á–µ—Ç scale_pos_weight
        scale_pos_weight = self.calculate_scale_pos_weight(y_train)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_space = self.get_hyperparameter_space(scale_pos_weight)
        
        # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
        base_model = xgb.XGBClassifier(
            objective='binary:logistic',
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        # –í—ã–±–æ—Ä scorer'–∞
        if self.precision_focused:
            scoring = self.precision_scorer
            logger.info("üìä Optimization metric: PRECISION")
        else:
            scoring = 'roc_auc'
            logger.info("üìä Optimization metric: ROC-AUC")
        
        # RandomizedSearchCV
        logger.info("üîç –ó–∞–ø—É—Å–∫ RandomizedSearchCV...")
        random_search = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_space,
            n_iter=50,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            scoring=scoring,
            cv=3,  # 3-fold CV
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        random_search.fit(X_train, y_train)
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        self.model = random_search.best_estimator_
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ validation set
        y_valid_pred = self.model.predict(X_valid)
        y_valid_proba = self.model.predict_proba(X_valid)[:, 1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ validation
        valid_precision = precision_score(y_valid, y_valid_pred)
        valid_recall = recall_score(y_valid, y_valid_pred)
        valid_f1 = f1_score(y_valid, y_valid_pred)
        valid_roc_auc = roc_auc_score(y_valid, y_valid_proba)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã tuning'–∞
        tuning_results = {
            'best_params': random_search.best_params_,
            'best_cv_score': float(random_search.best_score_),
            'cv_std': float(random_search.cv_results_['std_test_score'][random_search.best_index_]),
            'validation_metrics': {
                'precision': float(valid_precision),
                'recall': float(valid_recall),
                'f1_score': float(valid_f1),
                'roc_auc': float(valid_roc_auc)
            }
        }
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üèÜ Best CV score: {random_search.best_score_:.4f}")
        logger.info(f"üìä Valid metrics: Precision={valid_precision:.3f}, Recall={valid_recall:.3f}, F1={valid_f1:.3f}")
        
        self.training_history['tuning_results'] = tuning_results
        
        return tuning_results
    
    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series) -> dict:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ test set"""
        logger.info("üß™ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ test set...")
        
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ train_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_test_pred = self.model.predict(X_test)
        y_test_proba = self.model.predict_proba(X_test)[:, 1]
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        test_precision = precision_score(y_test, y_test_pred)
        test_recall = recall_score(y_test, y_test_pred)
        test_f1 = f1_score(y_test, y_test_pred)
        test_roc_auc = roc_auc_score(y_test, y_test_proba)
        test_pr_auc = average_precision_score(y_test, y_test_proba)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_test_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value
        
        test_results = {
            'confusion_matrix': {
                'TP': int(tp), 'FP': int(fp), 
                'TN': int(tn), 'FN': int(fn)
            },
            'metrics': {
                'precision': float(test_precision),
                'recall': float(test_recall),
                'f1_score': float(test_f1),
                'roc_auc': float(test_roc_auc),
                'pr_auc': float(test_pr_auc),
                'specificity': float(specificity),
                'npv': float(npv)
            },
            'business_impact': {
                'false_positive_rate': float(fp / (fp + tn)) if (fp + tn) > 0 else 0,
                'false_negative_rate': float(fn / (fn + tp)) if (fn + tp) > 0 else 0,
                'predicted_positive_rate': float((tp + fp) / len(y_test)),
                'actual_positive_rate': float(y_test.mean())
            }
        }
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê TEST SET:")
        logger.info(f"   Precision: {test_precision:.3f}")
        logger.info(f"   Recall: {test_recall:.3f}")
        logger.info(f"   F1-Score: {test_f1:.3f}")
        logger.info(f"   ROC-AUC: {test_roc_auc:.3f}")
        logger.info(f"   PR-AUC: {test_pr_auc:.3f}")
        logger.info(f"üìä Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}")
        logger.info(f"üíº False Positive Rate: {test_results['business_impact']['false_positive_rate']:.1%}")
        
        self.training_history['test_results'] = test_results
        
        return test_results

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø XGBOOST –ú–û–î–ï–õ–ò")
    logger.info("=" * 60)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = XGBoostTrainer(precision_focused=True)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_valid, X_test, y_train, y_valid, y_test = trainer.load_data()
        
        # –°–∫–µ–π–ª–∏–Ω–≥ —Ñ–∏—á–µ–π
        X_train_scaled = trainer.fit_scaler(X_train)
        X_valid_scaled = trainer.transform_features(X_valid)
        X_test_scaled = trainer.transform_features(X_test)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        tuning_results = trainer.train_model(X_train_scaled, y_train, X_valid_scaled, y_valid)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set
        test_results = trainer.evaluate_model(X_test_scaled, y_test)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        with open(f'training_history_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(trainer.training_history, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÅ –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: training_history_{timestamp}.json")
        logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
