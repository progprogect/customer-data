#!/usr/bin/env python3
"""
Create Churn Training Dataset Script
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ churn prediction

Author: Customer Data Analytics Team
"""

import os
import sys
import psycopg2
import logging
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('churn_training_dataset.log')
    ]
)
logger = logging.getLogger(__name__)


def execute_sql_file(conn: psycopg2.extensions.connection, file_path: Path) -> bool:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–∞"""
    try:
        logger.info(f"üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {file_path.name}...")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        with conn.cursor() as cursor:
            cursor.execute(sql_content)
            conn.commit()
            
        logger.info(f"‚úÖ {file_path.name} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {file_path.name}: {e}")
        conn.rollback()
        return False


def check_table_exists(conn: psycopg2.extensions.connection, table_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã"""
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                )
            """, (table_name,))
            return cursor.fetchone()[0]
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
        return False


def get_dataset_stats(conn: psycopg2.extensions.connection, table_name: str) -> dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(*) FILTER (WHERE split_type = 'train') as train_records,
                    COUNT(*) FILTER (WHERE split_type = 'valid_test') as valid_test_records,
                    COUNT(*) FILTER (WHERE target = TRUE) as churn_count,
                    COUNT(*) FILTER (WHERE target = FALSE) as retention_count,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT snapshot_date) as unique_snapshots,
                    MIN(snapshot_date) as earliest_snapshot,
                    MAX(snapshot_date) as latest_snapshot,
                    COUNT(*) FILTER (WHERE recency_days IS NULL) as recency_nulls,
                    COUNT(*) FILTER (WHERE frequency_90d IS NULL) as frequency_nulls,
                    COUNT(*) FILTER (WHERE monetary_180d IS NULL) as monetary_nulls,
                    COUNT(*) FILTER (WHERE aov_180d IS NULL) as aov_nulls
                FROM {table_name}
            """)
            
            result = cursor.fetchone()
            return {
                'total_records': result[0],
                'train_records': result[1],
                'valid_test_records': result[2],
                'churn_count': result[3],
                'retention_count': result[4],
                'unique_users': result[5],
                'unique_snapshots': result[6],
                'earliest_snapshot': result[7],
                'latest_snapshot': result[8],
                'recency_nulls': result[9],
                'frequency_nulls': result[10],
                'monetary_nulls': result[11],
                'aov_nulls': result[12]
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ {table_name}: {e}")
        return {}


def get_feature_correlations(conn: psycopg2.extensions.connection, table_name: str) -> dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ç–∞—Ä–≥–µ—Ç–æ–º"""
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT 
                    CORR(COALESCE(recency_days, 999), target::int) as recency_corr,
                    CORR(frequency_90d, target::int) as frequency_corr,
                    CORR(monetary_180d, target::int) as monetary_corr,
                    CORR(COALESCE(aov_180d, 0), target::int) as aov_corr,
                    CORR(orders_lifetime, target::int) as orders_lifetime_corr,
                    CORR(revenue_lifetime, target::int) as revenue_lifetime_corr,
                    CORR(categories_unique, target::int) as categories_unique_corr
                FROM {table_name}
            """)
            
            result = cursor.fetchone()
            return {
                'recency_corr': result[0],
                'frequency_corr': result[1],
                'monetary_corr': result[2],
                'aov_corr': result[3],
                'orders_lifetime_corr': result[4],
                'revenue_lifetime_corr': result[5],
                'categories_unique_corr': result[6]
            }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π {table_name}: {e}")
        return {}


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –°–û–ó–î–ê–ù–ò–ï –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê CHURN PREDICTION")
    logger.info("=" * 60)
    
    # –ü—É—Ç—å –∫ SQL —Ñ–∞–π–ª–∞–º
    sql_dir = Path(__file__).parent.parent / 'sql'
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    sql_files = [
        'create_churn_training_table.sql',      # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        'create_churn_training_dataset.sql',    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        'analyze_churn_dataset.sql'             # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
    ]
    
    conn = None
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
        conn = psycopg2.connect(
            host="localhost",
            database="customer_data",
            user="mikitavalkunovich",
            port="5432"
        )
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        required_tables = ['ml_user_features_daily_all', 'ml_labels_churn_60d']
        for table in required_tables:
            if not check_table_exists(conn, table):
                logger.error(f"‚ùå –¢–∞–±–ª–∏—Ü–∞ {table} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                logger.error("   –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ñ–∏—á –∏ –ª–µ–π–±–ª–æ–≤")
                return False
            logger.info(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {table} –Ω–∞–π–¥–µ–Ω–∞")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for i, sql_file in enumerate(sql_files, 1):
            file_path = sql_dir / sql_file
            
            if not file_path.exists():
                logger.error(f"‚ùå SQL —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                continue
                
            logger.info(f"üìã –®–∞–≥ {i}/{len(sql_files)}: {sql_file}")
            success = execute_sql_file(conn, file_path)
            
            if not success:
                logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {sql_file}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                return False
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        stats = get_dataset_stats(conn, 'ml_training_dataset_churn')
        correlations = get_feature_correlations(conn, 'ml_training_dataset_churn')
        
        if stats:
            churn_rate = (stats['churn_count'] / stats['total_records']) * 100 if stats['total_records'] > 0 else 0
            train_ratio = (stats['train_records'] / stats['total_records']) * 100 if stats['total_records'] > 0 else 0
            null_rate = ((stats['recency_nulls'] + stats['frequency_nulls'] + stats['monetary_nulls'] + stats['aov_nulls']) / stats['total_records']) * 100 if stats['total_records'] > 0 else 0
            
            logger.info("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–ó–î–ê–ù–ò–Ø –î–ê–¢–ê–°–ï–¢–ê:")
            logger.info("=" * 50)
            logger.info(f"üìã –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {stats['total_records']:,}")
            logger.info(f"üë• –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['unique_users']:,}")
            logger.info(f"üìÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤: {stats['unique_snapshots']}")
            logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {stats['earliest_snapshot']} - {stats['latest_snapshot']}")
            logger.info(f"")
            logger.info(f"üéØ SPLIT –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   Train: {stats['train_records']:,} ({train_ratio:.1f}%)")
            logger.info(f"   Valid/Test: {stats['valid_test_records']:,} ({100-train_ratio:.1f}%)")
            logger.info(f"")
            logger.info(f"üíî Churn —Å–ª—É—á–∞–µ–≤: {stats['churn_count']:,} ({churn_rate:.1f}%)")
            logger.info(f"üíö Retention —Å–ª—É—á–∞–µ–≤: {stats['retention_count']:,} ({100-churn_rate:.1f}%)")
            logger.info(f"")
            logger.info(f"‚ùì NULL –∑–Ω–∞—á–µ–Ω–∏—è:")
            logger.info(f"   Recency: {stats['recency_nulls']:,}")
            logger.info(f"   Frequency: {stats['frequency_nulls']:,}")
            logger.info(f"   Monetary: {stats['monetary_nulls']:,}")
            logger.info(f"   AOV: {stats['aov_nulls']:,}")
            logger.info(f"   –û–±—â–∏–π NULL rate: {null_rate:.1f}%")
            
            if correlations:
                logger.info(f"")
                logger.info(f"üîó –ö–û–†–†–ï–õ–Ø–¶–ò–ò –° CHURN:")
                logger.info(f"   Recency: {correlations['recency_corr']:.4f}")
                logger.info(f"   Frequency: {correlations['frequency_corr']:.4f}")
                logger.info(f"   Monetary: {correlations['monetary_corr']:.4f}")
                logger.info(f"   AOV: {correlations['aov_corr']:.4f}")
                logger.info(f"   Orders Lifetime: {correlations['orders_lifetime_corr']:.4f}")
                logger.info(f"   Revenue Lifetime: {correlations['revenue_lifetime_corr']:.4f}")
                logger.info(f"   Categories Unique: {correlations['categories_unique_corr']:.4f}")
            
            logger.info("=" * 50)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            quality_issues = []
            if churn_rate < 15 or churn_rate > 50:
                quality_issues.append(f"Churn rate ({churn_rate:.1f}%) –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 15-50%")
            if train_ratio < 65 or train_ratio > 75:
                quality_issues.append(f"Train ratio ({train_ratio:.1f}%) –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 65-75%")
            if null_rate > 5:
                quality_issues.append(f"NULL rate ({null_rate:.1f}%) —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π")
            
            if not quality_issues:
                logger.info("‚úÖ –û–¢–õ–ò–ß–ù–û–ï –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•!")
            else:
                logger.warning("‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´:")
                for issue in quality_issues:
                    logger.warning(f"   - {issue}")
        
        logger.info("üéâ –¢–†–ï–ù–ò–†–û–í–û–ß–ù–´–ô –î–ê–¢–ê–°–ï–¢ CHURN PREDICTION –°–û–ó–î–ê–ù –£–°–ü–ï–®–ù–û!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False
        
    finally:
        if conn:
            conn.close()
            logger.info("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
