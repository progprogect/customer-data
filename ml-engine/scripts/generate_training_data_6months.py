#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generate Training Data for XGBoost (6 Months Weekly)
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è XGBoost —Å –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–º–∏ —Å–Ω–∞–ø—à–æ—Ç–∞–º–∏ –∑–∞ 6 –º–µ—Å—è—Ü–µ–≤

Author: Customer Data Analytics Team
"""

import psycopg2
import logging
import sys
import os
from datetime import datetime
from typing import Dict, Any
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('generate_training_data.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'dbname': 'customer_data',
    'user': 'mikitavalkunovich',
    'password': '',
    'port': 5432
}

def connect_to_db() -> psycopg2.extensions.connection:
    """
    –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        psycopg2.connection: –û–±—ä–µ–∫—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return conn
    except psycopg2.Error as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def execute_sql_file(conn: psycopg2.extensions.connection, file_path: str) -> bool:
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–∞
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        file_path: –ü—É—Ç—å –∫ SQL —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            sql_content = file.read()
        
        with conn.cursor() as cursor:
            cursor.execute(sql_content)
            conn.commit()
            
        logger.info(f"‚úÖ SQL —Ñ–∞–π–ª {file_path} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL —Ñ–∞–π–ª–∞ {file_path}: {e}")
        conn.rollback()
        return False

def validate_generated_data(conn: psycopg2.extensions.connection) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        
    Returns:
        Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    validation_metrics = {}
    
    try:
        with conn.cursor() as cursor:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏—Ç—Ä–∏–Ω—É —Ñ–∏—á
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_features,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT snapshot_date) as unique_snapshots,
                    MIN(snapshot_date) as min_date,
                    MAX(snapshot_date) as max_date
                FROM ml_user_features_daily_all
            """)
            features_stats = cursor.fetchone()
            
            validation_metrics['features'] = {
                'total_rows': features_stats[0],
                'unique_users': features_stats[1],
                'unique_snapshots': features_stats[2],
                'min_date': features_stats[3],
                'max_date': features_stats[4]
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_labels,
                    COUNT(CASE WHEN purchase_next_30d = TRUE THEN 1 END) as positive_class,
                    COUNT(CASE WHEN purchase_next_30d = FALSE THEN 1 END) as negative_class,
                    ROUND(
                        COUNT(CASE WHEN purchase_next_30d = TRUE THEN 1 END)::NUMERIC / 
                        COUNT(*)::NUMERIC * 100, 2
                    ) as positive_class_percent,
                    MIN(snapshot_date) as min_date,
                    MAX(snapshot_date) as max_date
                FROM ml_labels_purchase_30d
            """)
            labels_stats = cursor.fetchone()
            
            validation_metrics['labels'] = {
                'total_rows': labels_stats[0],
                'positive_class': labels_stats[1],
                'negative_class': labels_stats[2],
                'positive_class_percent': float(labels_stats[3]) if labels_stats[3] else 0,
                'min_date': labels_stats[4],
                'max_date': labels_stats[5]
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∏—á –∏ –ª–µ–π–±–ª–æ–≤
            cursor.execute("""
                SELECT COUNT(*) 
                FROM ml_user_features_daily_all f
                INNER JOIN ml_labels_purchase_30d l 
                    ON f.user_id = l.user_id 
                    AND f.snapshot_date = l.snapshot_date
            """)
            matching_rows = cursor.fetchone()[0]
            validation_metrics['matching_rows'] = matching_rows
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        validation_metrics['error'] = str(e)
    
    return validation_metrics

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è XGBoost (6 –º–µ—Å—è—Ü–µ–≤, –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ)")
    
    # –ü—É—Ç—å –∫ SQL —Ñ–∞–π–ª–∞–º
    sql_dir = os.path.join(os.path.dirname(__file__), '..', 'sql')
    
    sql_files = [
        'create_target_labels.sql',                    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ç–∞—Ä–≥–µ—Ç–æ–≤
        'compute_ml_user_features_6months_weekly_fixed.sql', # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á –∑–∞ 6 –º–µ—Å—è—Ü–µ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è)
        'populate_buyers_features.sql',               # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π
        'log_features_stats.sql',                     # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–∏—á
        'generate_target_labels_6months_fixed.sql',   # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞—Ä–≥–µ—Ç–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è)
        'log_target_stats.sql'                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤
    ]
    
    conn = None
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        conn = connect_to_db()
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for sql_file in sql_files:
            file_path = os.path.join(sql_dir, sql_file)
            
            if not os.path.exists(file_path):
                logger.error(f"‚ùå SQL —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                continue
                
            logger.info(f"üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {sql_file}...")
            success = execute_sql_file(conn, file_path)
            
            if not success:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {sql_file}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                return False
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        validation_metrics = validate_generated_data(conn)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò:")
        logger.info("=" * 50)
        
        if 'features' in validation_metrics:
            f = validation_metrics['features']
            logger.info(f"üîß –í–ò–¢–†–ò–ù–ê –§–ò–ß:")
            logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {f['total_rows']:,}")
            logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {f['unique_users']:,}")
            logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤: {f['unique_snapshots']}")
            logger.info(f"  ‚Ä¢ –ü–µ—Ä–∏–æ–¥: {f['min_date']} ‚Äî {f['max_date']}")
        
        if 'labels' in validation_metrics:
            l = validation_metrics['labels']
            logger.info(f"üéØ –¢–ê–†–ì–ï–¢–´:")
            logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {l['total_rows']:,}")
            logger.info(f"  ‚Ä¢ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {l['positive_class']:,}")
            logger.info(f"  ‚Ä¢ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {l['negative_class']:,}")
            logger.info(f"  ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞: {l['positive_class_percent']:.2f}%")
            logger.info(f"  ‚Ä¢ –ü–µ—Ä–∏–æ–¥: {l['min_date']} ‚Äî {l['max_date']}")
        
        if 'matching_rows' in validation_metrics:
            logger.info(f"üîó –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï:")
            logger.info(f"  ‚Ä¢ –°—Ç—Ä–æ–∫ —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º–∏ —Ñ–∏—á–∞–º–∏ –∏ —Ç–∞—Ä–≥–µ—Ç–∞–º–∏: {validation_metrics['matching_rows']:,}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("‚úÖ –ü–†–û–í–ï–†–ö–ò –ö–ê–ß–ï–°–¢–í–ê:")
        
        if 'labels' in validation_metrics:
            percent = validation_metrics['labels']['positive_class_percent']
            if 5 <= percent <= 30:
                logger.info(f"  ‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –Ω–æ—Ä–º–µ: {percent:.2f}%")
            else:
                logger.warning(f"  ‚ö†Ô∏è –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤–Ω–µ –Ω–æ—Ä–º—ã (5-30%): {percent:.2f}%")
        
        if 'features' in validation_metrics and 'labels' in validation_metrics:
            if validation_metrics['features']['total_rows'] == validation_metrics['labels']['total_rows']:
                logger.info("  ‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∏—á–∞—Ö –∏ —Ç–∞—Ä–≥–µ—Ç–∞—Ö —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            else:
                logger.warning("  ‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∏—á–∞—Ö –∏ —Ç–∞—Ä–≥–µ—Ç–∞—Ö –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ —Ñ–∞–π–ª
        with open('training_data_metrics.json', 'w', encoding='utf-8') as f:
            json.dump(validation_metrics, f, indent=2, default=str, ensure_ascii=False)
        
        logger.info("üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info("üìÅ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ training_data_metrics.json")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False
        
    finally:
        if conn:
            conn.close()
            logger.info("üîê –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
