#!/usr/bin/env python3
"""
Train XGBoost Churn Prediction Model
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤

Author: Customer Data Analytics Team
"""

import os
import sys
import psycopg2
import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
import logging
from datetime import datetime
from pathlib import Path
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('churn_xgboost_training.log')
    ]
)
logger = logging.getLogger(__name__)


def load_training_data(conn: psycopg2.extensions.connection) -> tuple:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã"""
    try:
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train –¥–∞–Ω–Ω—ã–µ
        train_query = """
        SELECT 
            recency_days, frequency_90d, monetary_180d, aov_180d,
            orders_lifetime, revenue_lifetime, categories_unique, target
        FROM ml_training_dataset_churn 
        WHERE split_type = 'train'
        ORDER BY user_id, snapshot_date
        """
        
        train_data = pd.read_sql(train_query, conn)
        logger.info(f"‚úÖ Train –¥–∞–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º valid/test –¥–∞–Ω–Ω—ã–µ
        valid_query = """
        SELECT 
            recency_days, frequency_90d, monetary_180d, aov_180d,
            orders_lifetime, revenue_lifetime, categories_unique, target
        FROM ml_training_dataset_churn 
        WHERE split_type = 'valid_test'
        ORDER BY user_id, snapshot_date
        """
        
        valid_data = pd.read_sql(valid_query, conn)
        logger.info(f"‚úÖ Valid/Test –¥–∞–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(valid_data)} –∑–∞–ø–∏—Å–µ–π")
        
        return train_data, valid_data
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise


def prepare_features_and_target(data: pd.DataFrame) -> tuple:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ç–∞—Ä–≥–µ—Ç–∞"""
    try:
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ (X)
        feature_columns = [
            'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
            'orders_lifetime', 'revenue_lifetime', 'categories_unique'
        ]
        
        X = data[feature_columns].copy()
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NULL –∑–Ω–∞—á–µ–Ω–∏—è
        X['recency_days'] = X['recency_days'].fillna(999)  # –ï—Å–ª–∏ –Ω–µ –ø–æ–∫—É–ø–∞–ª - —Å—Ç–∞–≤–∏–º –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        X['aov_180d'] = X['aov_180d'].fillna(0)  # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –∑–∞–∫–∞–∑–æ–≤ - 0
        
        # –¢–∞—Ä–≥–µ—Ç (y) - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º boolean –≤ int
        y = data['target'].astype(int)
        
        logger.info(f"üìã –ü—Ä–∏–∑–Ω–∞–∫–∏: {list(X.columns)}")
        logger.info(f"üìä –†–∞–∑–º–µ—Ä X: {X.shape}, —Ä–∞–∑–º–µ—Ä y: {y.shape}")
        logger.info(f"üéØ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")
        
        return X, y
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise


def get_scale_pos_weight(y_train: pd.Series) -> float:
    """–†–∞—Å—á–µ—Ç –≤–µ—Å–∞ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (churn)"""
    churn_count = y_train.sum()
    retention_count = len(y_train) - churn_count
    scale_pos_weight = retention_count / churn_count
    logger.info(f"‚öñÔ∏è Scale pos weight: {scale_pos_weight:.2f} (retention: {retention_count}, churn: {churn_count})")
    return scale_pos_weight


def train_xgboost_model(X_train: pd.DataFrame, y_train: pd.Series, 
                       X_valid: pd.DataFrame, y_valid: pd.Series) -> xgb.XGBClassifier:
    """–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏"""
    try:
        logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–∏...")
        
        # –†–∞—Å—á–µ—Ç –≤–µ—Å–∞ –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        scale_pos_weight = get_scale_pos_weight(y_train)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'n_estimators': 300,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'scale_pos_weight': scale_pos_weight,
            'random_state': 42,
            'n_jobs': -1
        }
        
        logger.info(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {params}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = xgb.XGBClassifier(**params)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
        
        return model
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        raise


def evaluate_model(model: xgb.XGBClassifier, X_valid: pd.DataFrame, y_valid: pd.Series) -> dict:
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    try:
        logger.info("üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict_proba(X_valid)[:, 1]
        y_pred = model.predict(X_valid)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        auc = roc_auc_score(y_valid, y_pred_proba)
        precision = precision_score(y_valid, y_pred)
        recall = recall_score(y_valid, y_pred)
        f1 = f1_score(y_valid, y_pred)
        
        # Confusion Matrix
        cm = confusion_matrix(y_valid, y_pred)
        
        metrics = {
            'auc': auc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm,
            'y_pred_proba': y_pred_proba,
            'y_pred': y_pred
        }
        
        logger.info("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
        logger.info(f"   AUC-ROC: {auc:.4f}")
        logger.info(f"   Precision: {precision:.4f}")
        logger.info(f"   Recall: {recall:.4f}")
        logger.info(f"   F1-Score: {f1:.4f}")
        logger.info(f"   Confusion Matrix:\n{cm}")
        
        return metrics
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise


def get_feature_importance(model: xgb.XGBClassifier, feature_names: list) -> pd.DataFrame:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    try:
        importance_scores = model.feature_importances_
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=False)
        
        logger.info("üîç –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for _, row in importance_df.iterrows():
            logger.info(f"   {row['feature']}: {row['importance']:.4f}")
        
        return importance_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        raise


def save_model_and_report(model: xgb.XGBClassifier, metrics: dict, 
                         feature_importance: pd.DataFrame, 
                         output_dir: Path) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –æ—Ç—á–µ—Ç–∞"""
    try:
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –æ—Ç—á–µ—Ç–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = output_dir / 'churn_xgboost_model.pkl'
        joblib.dump(model, model_path)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report_path = output_dir / 'churn_model_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("CHURN PREDICTION MODEL REPORT\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–æ–¥–µ–ª—å: XGBoost Classifier\n\n")
            
            f.write("–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê:\n")
            f.write(f"AUC-ROC: {metrics['auc']:.4f}\n")
            f.write(f"Precision: {metrics['precision']:.4f}\n")
            f.write(f"Recall: {metrics['recall']:.4f}\n")
            f.write(f"F1-Score: {metrics['f1']:.4f}\n\n")
            
            f.write("CONFUSION MATRIX:\n")
            f.write(f"{metrics['confusion_matrix']}\n\n")
            
            f.write("–í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í:\n")
            for _, row in feature_importance.iterrows():
                f.write(f"{row['feature']}: {row['importance']:.4f}\n")
        
        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        raise


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –û–ë–£–ß–ï–ù–ò–ï XGBOOST –ú–û–î–ï–õ–ò CHURN PREDICTION")
    logger.info("=" * 60)
    
    conn = None
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
        conn = psycopg2.connect(
            host="localhost",
            database="customer_data",
            user="mikitavalkunovich",
            port="5432"
        )
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_data, valid_data = load_training_data(conn)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ç–∞—Ä–≥–µ—Ç–∞
        X_train, y_train = prepare_features_and_target(train_data)
        X_valid, y_valid = prepare_features_and_target(valid_data)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = train_xgboost_model(X_train, y_train, X_valid, y_valid)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        metrics = evaluate_model(model, X_valid, y_valid)
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = get_feature_importance(model, list(X_train.columns))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_dir = Path(__file__).parent.parent / 'models'
        save_model_and_report(model, metrics, feature_importance, output_dir)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        logger.info("üéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        logger.info("=" * 40)
        logger.info(f"üìä AUC-ROC: {metrics['auc']:.4f}")
        logger.info(f"üéØ Precision: {metrics['precision']:.4f}")
        logger.info(f"üîÑ Recall: {metrics['recall']:.4f}")
        logger.info(f"‚öñÔ∏è F1-Score: {metrics['f1']:.4f}")
        logger.info(f"üîç –¢–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–∞:")
        for i, (_, row) in enumerate(feature_importance.head(3).iterrows()):
            logger.info(f"   {i+1}. {row['feature']}: {row['importance']:.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if metrics['auc'] >= 0.7:
            logger.info("‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (AUC >= 0.7)")
        elif metrics['auc'] >= 0.6:
            logger.info("‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (AUC >= 0.6)")
        else:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è (AUC < 0.6)")
        
        logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï XGBOOST –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False
        
    finally:
        if conn:
            conn.close()
            logger.info("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
