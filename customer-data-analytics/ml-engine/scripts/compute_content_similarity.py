#!/usr/bin/env python3
"""
Content-Based Item Similarity Computation
–í—ã—á–∏—Å–ª–µ–Ω–∏–µ content-based –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ —Ç–æ–≤–∞—Ä–æ–≤

Author: Customer Data Analytics Team
"""

import os
import sys
import numpy as np
import pandas as pd
import psycopg2
import json
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from scipy.sparse import csr_matrix, hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('content_similarity.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
SIMILARITY_CONFIG = {
    'weights': {
        'tfidf_tags': 0.4,
        'brand': 0.2,
        'category': 0.1,
        'style': 0.1,  # –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º
        'numeric': 0.3
    },
    'tfidf': {
        'max_features': 500,  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        'min_df': 2,         # –º–∏–Ω–∏–º—É–º 2 —Ç–æ–≤–∞—Ä–∞ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å —Ç–µ–≥
        'max_df': 0.8,       # –∏—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ
        'ngram_range': (1, 1)
    },
    'numeric_features': ['price_current', 'popularity_30d', 'rating'],
    'top_k': 50,             # —Ç–æ–ø-50 –ø–æ—Ö–æ–∂–∏—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
    'min_sim_score': 0.01    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
}

class ContentSimilarityComputer:
    def __init__(self, db_connection_string: str):
        self.db_conn_str = db_connection_string
        self.conn = None
        self.products_df = None
        self.feature_vectors = None
        self.similarity_matrix = None
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        try:
            self.conn = psycopg2.connect(self.db_conn_str)
            logger.info("‚úÖ Connected to database")
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {e}")
            raise
    
    def load_product_features(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –ë–î"""
        query = """
        SELECT 
            product_id,
            brand,
            category,
            color,
            size,
            material,
            gender,
            style,
            price_current,
            rating,
            popularity_30d,
            tags_normalized,
            tags_count,
            title,
            description_short,
            is_active
        FROM ml_item_content_features
        WHERE is_active = true
        ORDER BY product_id
        """
        
        logger.info("üìä Loading product features from database...")
        self.products_df = pd.read_sql(query, self.conn)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self.products_df['brand'] = self.products_df['brand'].fillna('unknown')
        self.products_df['category'] = self.products_df['category'].fillna('unknown')
        self.products_df['style'] = self.products_df['style'].fillna('unknown')
        self.products_df['rating'] = self.products_df['rating'].fillna(2.5)
        self.products_df['popularity_30d'] = self.products_df['popularity_30d'].fillna(0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–≥–æ–≤
        self.products_df['tags_text'] = self.products_df['tags_normalized'].apply(
            lambda x: ' '.join(x) if x and isinstance(x, list) else ''
        )
        
        logger.info(f"üìà Loaded {len(self.products_df)} active products")
        return self.products_df
    
    def create_tfidf_features(self) -> csr_matrix:
        """–°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–µ–≥–æ–≤"""
        logger.info("üî§ Creating TF-IDF vectors for tags...")
        
        vectorizer = TfidfVectorizer(
            max_features=SIMILARITY_CONFIG['tfidf']['max_features'],
            min_df=SIMILARITY_CONFIG['tfidf']['min_df'],
            max_df=SIMILARITY_CONFIG['tfidf']['max_df'],
            ngram_range=SIMILARITY_CONFIG['tfidf']['ngram_range'],
            token_pattern=r'\b[a-zA-Z]+\b',  # —Ç–æ–ª—å–∫–æ –±—É–∫–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            lowercase=True,
            stop_words=None
        )
        
        # –°–æ–∑–¥–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
        tfidf_matrix = vectorizer.fit_transform(self.products_df['tags_text'])
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        feature_names = vectorizer.get_feature_names_out()
        logger.info(f"üìä TF-IDF vocabulary size: {len(feature_names)}")
        logger.info(f"üìä TF-IDF matrix shape: {tfidf_matrix.shape}")
        logger.info(f"üìä TF-IDF sparsity: {(1 - tfidf_matrix.nnz / tfidf_matrix.size) * 100:.1f}%")
        
        # –¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ç–µ–≥–æ–≤
        feature_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()
        top_features_idx = np.argsort(feature_scores)[-20:]
        top_features = [feature_names[i] for i in top_features_idx[::-1]]
        logger.info(f"üè∑Ô∏è Top tags: {', '.join(top_features[:10])}")
        
        return tfidf_matrix, vectorizer
    
    def create_categorical_features(self) -> csr_matrix:
        """–°–æ–∑–¥–∞–Ω–∏–µ one-hot –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üìÇ Creating categorical features...")
        
        categorical_matrices = []
        
        # Brand features
        brand_encoder = LabelEncoder()
        brand_encoded = brand_encoder.fit_transform(self.products_df['brand'])
        brand_matrix = self._create_onehot_matrix(brand_encoded, len(brand_encoder.classes_))
        categorical_matrices.append(brand_matrix)
        logger.info(f"üè∑Ô∏è Brand features: {len(brand_encoder.classes_)} unique brands")
        
        # Category features  
        category_encoder = LabelEncoder()
        category_encoded = category_encoder.fit_transform(self.products_df['category'])
        category_matrix = self._create_onehot_matrix(category_encoded, len(category_encoder.classes_))
        categorical_matrices.append(category_matrix)
        logger.info(f"üìÇ Category features: {len(category_encoder.classes_)} unique categories")
        
        # Style features
        style_encoder = LabelEncoder()
        style_encoded = style_encoder.fit_transform(self.products_df['style'])
        style_matrix = self._create_onehot_matrix(style_encoded, len(style_encoder.classes_))
        categorical_matrices.append(style_matrix)
        logger.info(f"üé® Style features: {len(style_encoder.classes_)} unique styles")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        categorical_features = hstack(categorical_matrices)
        logger.info(f"üìä Total categorical features shape: {categorical_features.shape}")
        
        return categorical_features
    
    def _create_onehot_matrix(self, encoded_values: np.ndarray, n_classes: int) -> csr_matrix:
        """–°–æ–∑–¥–∞–Ω–∏–µ one-hot –º–∞—Ç—Ä–∏—Ü—ã"""
        from scipy.sparse import csr_matrix
        rows = np.arange(len(encoded_values))
        cols = encoded_values
        data = np.ones(len(encoded_values))
        return csr_matrix((data, (rows, cols)), shape=(len(encoded_values), n_classes))
    
    def create_numeric_features(self) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üî¢ Creating numeric features...")
        
        numeric_cols = SIMILARITY_CONFIG['numeric_features']
        numeric_data = self.products_df[numeric_cols].copy()
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ü–µ–Ω—ã –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        numeric_data['price_current'] = np.log1p(numeric_data['price_current'])
        numeric_data['popularity_30d'] = np.log1p(numeric_data['popularity_30d'])
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        numeric_features = scaler.fit_transform(numeric_data)
        
        logger.info(f"üìä Numeric features shape: {numeric_features.shape}")
        logger.info(f"üìä Feature means: {numeric_features.mean(axis=0)}")
        logger.info(f"üìä Feature stds: {numeric_features.std(axis=0)}")
        
        return numeric_features
    
    def combine_features(self) -> Tuple[csr_matrix, Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤–µ—Å–∞–º–∏"""
        logger.info("üîó Combining all feature types...")
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        tfidf_matrix, tfidf_vectorizer = self.create_tfidf_features()
        categorical_matrix = self.create_categorical_features()
        numeric_matrix = self.create_numeric_features()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞
        weights = SIMILARITY_CONFIG['weights']
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º TF-IDF
        tfidf_weighted = tfidf_matrix * weights['tfidf_tags']
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å–∞)
        cat_weight_per_feature = (weights['brand'] + weights['category'] + weights['style']) / categorical_matrix.shape[1]
        categorical_weighted = categorical_matrix * cat_weight_per_feature
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ
        numeric_weight_per_feature = weights['numeric'] / numeric_matrix.shape[1]
        numeric_weighted = numeric_matrix * numeric_weight_per_feature
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–Ω—É –º–∞—Ç—Ä–∏—Ü—É
        from scipy.sparse import csr_matrix
        numeric_sparse = csr_matrix(numeric_weighted)
        
        combined_features = hstack([tfidf_weighted, categorical_weighted, numeric_sparse])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        feature_info = {
            'tfidf_features': tfidf_matrix.shape[1],
            'categorical_features': categorical_matrix.shape[1], 
            'numeric_features': numeric_matrix.shape[1],
            'total_features': combined_features.shape[1],
            'vectorizer': tfidf_vectorizer
        }
        
        logger.info(f"üéØ Combined features shape: {combined_features.shape}")
        logger.info(f"üìä Feature breakdown: TF-IDF={feature_info['tfidf_features']}, "
                   f"Categorical={feature_info['categorical_features']}, "
                   f"Numeric={feature_info['numeric_features']}")
        
        self.feature_vectors = combined_features
        return combined_features, feature_info
    
    def compute_similarity_matrix(self, features: csr_matrix) -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ cosine similarity –º–∞—Ç—Ä–∏—Ü—ã"""
        logger.info("üîÑ Computing cosine similarity matrix...")
        
        n_items = features.shape[0]
        logger.info(f"üìä Computing similarity for {n_items} items...")
        
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á–∏
        batch_size = 100
        similarity_matrix = np.zeros((n_items, n_items), dtype=np.float32)
        
        for i in range(0, n_items, batch_size):
            end_i = min(i + batch_size, n_items)
            batch_features = features[i:end_i]
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–æ–ª—å–∫–æ —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ (—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞)
            for j in range(i, n_items, batch_size):
                end_j = min(j + batch_size, n_items)
                other_features = features[j:end_j]
                
                # Cosine similarity
                sim_batch = cosine_similarity(batch_features, other_features)
                similarity_matrix[i:end_i, j:end_j] = sim_batch
                
                # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—É—é —á–∞—Å—Ç—å
                if i != j:
                    similarity_matrix[j:end_j, i:end_i] = sim_batch.T
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"‚è≥ Processed {i + batch_size}/{n_items} items...")
        
        logger.info("‚úÖ Similarity matrix computation completed")
        self.similarity_matrix = similarity_matrix
        return similarity_matrix
    
    def extract_top_similar(self, similarity_matrix: np.ndarray) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ø-K –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ"""
        logger.info(f"üîù Extracting top-{SIMILARITY_CONFIG['top_k']} similar items...")
        
        similarities = []
        n_items = similarity_matrix.shape[0]
        
        for i in range(n_items):
            product_id = self.products_df.iloc[i]['product_id']
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–≤–∞—Ä–∞
            sim_scores = similarity_matrix[i]
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–µ–±—è –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            sim_scores[i] = -1  # –∏—Å–∫–ª—é—á–∞–µ–º —Å–∞–º–æ–≥–æ —Å–µ–±—è
            top_indices = np.argsort(sim_scores)[::-1]
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø-K —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥—É
            count = 0
            for idx in top_indices:
                if count >= SIMILARITY_CONFIG['top_k']:
                    break
                
                sim_score = sim_scores[idx]
                if sim_score < SIMILARITY_CONFIG['min_sim_score']:
                    continue
                
                similar_product_id = self.products_df.iloc[idx]['product_id']
                
                similarities.append({
                    'product_id': int(product_id),
                    'similar_product_id': int(similar_product_id),
                    'sim_score': float(sim_score),
                    'features_used': 'tfidf_tags,brand,category,style,numeric'
                })
                count += 1
        
        logger.info(f"üìä Generated {len(similarities)} similarity pairs")
        return similarities
    
    def save_similarities_to_db(self, similarities: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î"""
        logger.info("üíæ Saving similarities to database...")
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        with self.conn.cursor() as cur:
            cur.execute("DELETE FROM ml_item_sim_content")
            logger.info("üóëÔ∏è Cleared old similarity data")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è batch insert
        insert_data = []
        for sim in similarities:
            insert_data.append((
                sim['product_id'],
                sim['similar_product_id'], 
                sim['sim_score'],
                sim['features_used'],
                json.dumps({})  # –ø–æ–∫–∞ –ø—É—Å—Ç–æ–π breakdown
            ))
        
        # Batch insert
        insert_query = """
        INSERT INTO ml_item_sim_content 
        (product_id, similar_product_id, sim_score, features_used, sim_breakdown)
        VALUES (%s, %s, %s, %s, %s)
        """
        
        with self.conn.cursor() as cur:
            cur.executemany(insert_query, insert_data)
            self.conn.commit()
        
        logger.info(f"‚úÖ Saved {len(similarities)} similarity records to database")
    
    def run_content_similarity_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è content-based —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        logger.info("üöÄ Starting content-based similarity computation pipeline...")
        start_time = datetime.now()
        
        try:
            # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.connect_db()
            self.load_product_features()
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features, feature_info = self.combine_features()
            
            # 3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarity_matrix = self.compute_similarity_matrix(features)
            
            # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ø-K
            similarities = self.extract_top_similar(similarity_matrix)
            
            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            self.save_similarities_to_db(similarities)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("üéâ Content similarity pipeline completed successfully!")
            logger.info(f"‚è±Ô∏è Total duration: {duration}")
            logger.info(f"üìä Products processed: {len(self.products_df)}")
            logger.info(f"üìä Similarity pairs generated: {len(similarities)}")
            logger.info(f"üìä Average similarities per product: {len(similarities) / len(self.products_df):.1f}")
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    db_connection = "postgresql://mikitavalkunovich@localhost:5432/customer_data"
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä —Å—Ö–æ–¥—Å—Ç–≤–∞
    computer = ContentSimilarityComputer(db_connection)
    computer.run_content_similarity_pipeline()


if __name__ == "__main__":
    main()
