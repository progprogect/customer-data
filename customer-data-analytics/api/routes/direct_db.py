"""
Direct Database Access for Frontend
–ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
"""

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
import logging
import os
import psycopg2
from datetime import datetime

logger = logging.getLogger(__name__)
router = APIRouter()


def get_db_connection():
    """–ü—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
    try:
        conn = psycopg2.connect(
            host="localhost",
            database="customer_data",
            user="mikitavalkunovich",
            port=5432
        )
        return conn
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
        raise


@router.get("/database-stats")
async def get_database_statistics() -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É
        cursor.execute("SELECT MAX(snapshot_date) FROM ml_user_features_daily_all")
        max_date = cursor.fetchone()[0]
        snapshot_date = max_date.isoformat() if max_date else "2025-09-15"
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_query = """
        SELECT 
            COUNT(*) as total_users,
            COUNT(*) FILTER (WHERE orders_lifetime >= 1) as users_with_orders,
            COUNT(*) FILTER (WHERE recency_days IS NOT NULL) as users_with_features
        FROM ml_user_features_daily_all 
        WHERE snapshot_date = %s
        """
        
        cursor.execute(stats_query, (snapshot_date,))
        total_users, users_with_orders, users_with_features = cursor.fetchone()
        
        cursor.close()
        conn.close()
        
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î: {total_users} –≤—Å–µ–≥–æ, {users_with_orders} —Å –∑–∞–∫–∞–∑–∞–º–∏")
        
        return {
            "snapshot_date": snapshot_date,
            "total_users_in_db": int(total_users),
            "users_with_orders": int(users_with_orders),
            "users_with_features": int(users_with_features),
            "sample_size_used": 500  # –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {str(e)}")


@router.get("/direct-users")
async def get_direct_users(limit: int = 500) -> Dict[str, Any]:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–æ 500
    """
    –ü—Ä—è–º–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É
        cursor.execute("SELECT MAX(snapshot_date) FROM ml_user_features_daily_all")
        max_date = cursor.fetchone()[0]
        snapshot_date = max_date.isoformat() if max_date else "2025-09-15"
        
        logger.info(f"üîç –ü–æ–ª—É—á–µ–Ω–∏–µ {limit} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è snapshot_date={snapshot_date}")
        
        # –ó–∞–ø—Ä–æ—Å —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (—Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏)
        query = """
        SELECT 
            f.user_id,
            COALESCE(f.recency_days, 0) as recency_days,
            COALESCE(f.frequency_90d, 0) as frequency_90d,
            COALESCE(f.monetary_180d, 0) as monetary_180d,
            COALESCE(f.aov_180d, 0) as aov_180d,
            COALESCE(f.orders_lifetime, 0) as orders_lifetime,
            COALESCE(f.revenue_lifetime, 0) as revenue_lifetime,
            COALESCE(f.categories_unique, 0) as categories_unique
        FROM ml_user_features_daily_all f
        WHERE f.snapshot_date = %s
          AND f.orders_lifetime >= 1
        ORDER BY RANDOM()  -- –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        LIMIT %s
        """
        
        cursor.execute(query, (snapshot_date, limit))
        rows = cursor.fetchall()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è ML API
        users_for_ml = []
        users_with_explanations = []
        
        for row in rows:
            user_id, recency, freq_90d, monetary_180d, aov_180d, orders_lifetime, revenue_lifetime, categories = row
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–ª—è ML API
            features = {
                "recency_days": float(recency),
                "frequency_90d": int(freq_90d),
                "monetary_180d": float(monetary_180d),
                "aov_180d": float(aov_180d),
                "orders_lifetime": int(orders_lifetime),
                "revenue_lifetime": float(revenue_lifetime),
                "categories_unique": int(categories)
            }
            
            user_for_ml = {
                "user_id": int(user_id),
                "snapshot_date": snapshot_date,
                "features": features
            }
            users_for_ml.append(user_for_ml)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            explanation = generate_explanation(features)
            
            user_with_explanation = {
                "user_id": int(user_id),
                "snapshot_date": snapshot_date,
                "features": features,
                "explanation": explanation
            }
            users_with_explanations.append(user_with_explanation)
        
        cursor.close()
        conn.close()
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(users_with_explanations)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        return {
            "users": users_with_explanations,
            "users_for_ml": users_for_ml,
            "total_count": len(users_with_explanations),
            "snapshot_date": snapshot_date
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}")


def generate_explanation(features: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞"""
    explanations = []
    
    # –ê–Ω–∞–ª–∏–∑ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏
    orders_lifetime = features.get('orders_lifetime', 0)
    if orders_lifetime >= 10:
        explanations.append(f"{orders_lifetime} –∑–∞–∫–∞–∑–æ–≤ –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è")
    elif orders_lifetime >= 5:
        explanations.append(f"–∞–∫—Ç–∏–≤–Ω—ã–π –∫–ª–∏–µ–Ω—Ç ({orders_lifetime} –∑–∞–∫–∞–∑–æ–≤)")
    elif orders_lifetime >= 2:
        explanations.append(f"–ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∫–ª–∏–µ–Ω—Ç ({orders_lifetime} –∑–∞–∫–∞–∑–∞)")
    
    # –ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–∞–≤–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    recency_days = features.get('recency_days', 999)
    if recency_days <= 30:
        explanations.append(f"–ø–æ–∫—É–ø–∞–ª –Ω–µ–¥–∞–≤–Ω–æ ({int(recency_days)} –¥–Ω–µ–π –Ω–∞–∑–∞–¥)")
    elif recency_days <= 90:
        explanations.append(f"–ø–æ–∫—É–ø–∞–ª –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –º–µ—Å—è—Ü–∞")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    categories_unique = features.get('categories_unique', 0)
    if categories_unique >= 5:
        explanations.append(f"–ø–æ–∫—É–ø–∞–µ—Ç –≤ {categories_unique} –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö")
    elif categories_unique >= 3:
        explanations.append(f"—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ç
    monetary_180d = features.get('monetary_180d', 0)
    if monetary_180d >= 1000:
        explanations.append(f"–ø–æ—Ç—Ä–∞—Ç–∏–ª ${int(monetary_180d)} –∑–∞ –ø–æ–ª–≥–æ–¥–∞")
    
    return ", ".join(explanations[:2]) if explanations else "–Ω–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç"
