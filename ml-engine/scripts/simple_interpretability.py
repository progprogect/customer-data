#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Model Interpretability Analysis
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è XGBoost –º–æ–¥–µ–ª–∏

Author: Customer Data Analytics Team
"""

import pandas as pd
import numpy as np
import json
import warnings
warnings.filterwarnings('ignore')

# ML libraries
import xgboost as xgb
import shap

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# System libraries
import logging
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è matplotlib
plt.switch_backend('Agg')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def safe_float(value):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float –¥–ª—è JSON"""
    if isinstance(value, (np.float32, np.float64)):
        return float(value)
    elif isinstance(value, (np.int32, np.int64)):
        return int(value)
    else:
        return value

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üîç –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ XGBoost –º–æ–¥–µ–ª–∏")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_df = pd.read_csv('train_set.csv')
        test_df = pd.read_csv('test_set.csv')
        
        feature_names = [
            'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
            'orders_lifetime', 'revenue_lifetime', 'categories_unique'
        ]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train = train_df[feature_names].fillna(train_df[feature_names].median())
        y_train = train_df['target']
        
        X_test = test_df[feature_names].fillna(train_df[feature_names].median())
        y_test = test_df['target']
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: train={len(X_train)}, test={len(X_test)}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            scale_pos_weight=1.57
        )
        
        model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        test_accuracy = model.score(X_test, y_test)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. Test accuracy: {test_accuracy:.3f}")
        
        # === FEATURE IMPORTANCE ANALYSIS ===
        
        # XGBoost feature importance
        importance_scores = model.feature_importances_
        importance_dict = {}
        
        for feature, score in zip(feature_names, importance_scores):
            importance_dict[feature] = safe_float(score)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("üèÜ –¢–û–ü-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (XGBoost):")
        for idx, (feature, importance) in enumerate(sorted_features[:5], 1):
            percentage = importance / sum(importance_dict.values()) * 100
            logger.info(f"   {idx}. {feature}: {importance:.3f} ({percentage:.1f}%)")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
        plt.figure(figsize=(10, 6))
        features = [f[0] for f in sorted_features]
        importances = [f[1] for f in sorted_features]
        
        sns.barplot(x=importances, y=features, palette='viridis')
        plt.title('Feature Importance (XGBoost)', fontsize=14, fontweight='bold')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å', fontsize=12)
        plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏', fontsize=12)
        plt.tight_layout()
        plt.savefig('feature_importance_simple.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: feature_importance_simple.png")
        
        # === SHAP ANALYSIS ===
        
        logger.info("üî¨ –ó–∞–ø—É—Å–∫ SHAP –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è SHAP
        X_sample = X_test.sample(n=min(300, len(X_test)), random_state=42)
        
        # SHAP explainer
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_sample)
        
        # SHAP feature importance (—Å—Ä–µ–¥–Ω–µ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        shap_importance = {}
        for idx, feature in enumerate(feature_names):
            mean_abs_shap = np.mean(np.abs(shap_values[:, idx]))
            shap_importance[feature] = safe_float(mean_abs_shap)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ SHAP –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_shap = sorted(shap_importance.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("üèÜ –¢–û–ü-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (SHAP):")
        for idx, (feature, shap_imp) in enumerate(sorted_shap[:5], 1):
            logger.info(f"   {idx}. {feature}: {shap_imp:.3f}")
        
        # SHAP summary plot
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values, X_sample, feature_names=feature_names, 
                         plot_type="bar", show=False)
        plt.tight_layout()
        plt.savefig('shap_importance_simple.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # SHAP detailed plot
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.savefig('shap_summary_simple.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("üìä SHAP –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        logger.info("   - shap_importance_simple.png")
        logger.info("   - shap_summary_simple.png")
        
        # === BUSINESS INSIGHTS ===
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_xgb = sorted_features[:3]
        top_shap = sorted_shap[:3]
        
        business_insights = {
            'xgb_top_feature': top_xgb[0][0],
            'shap_top_feature': top_shap[0][0],
            'consistency_check': top_xgb[0][0] == top_shap[0][0],
            'interpretation': {}
        }
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        interpretations = {
            'recency_days': "–î–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏ - –∫–ª—é—á–µ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –ø–æ–∫—É–ø–∫–µ",
            'frequency_90d': "–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫ –∑–∞ 90 –¥–Ω–µ–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞", 
            'monetary_180d': "–ü–æ—Ç—Ä–∞—á–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–∞ 180 –¥–Ω–µ–π - –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏",
            'aov_180d': "–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –∑–∞ 180 –¥–Ω–µ–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞",
            'orders_lifetime': "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤ - –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
            'revenue_lifetime': "–û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å lifetime value",
            'categories_unique': "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø–æ–∫—É–ø–æ–∫"
        }
        
        # –ë–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if top_xgb[0][0] == 'orders_lifetime':
            business_conclusion = "–ú–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ - —Ä–∞–∑–≤–∏–≤–∞–π—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã —É–¥–µ—Ä–∂–∞–Ω–∏—è"
        elif top_xgb[0][0] == 'categories_unique':
            business_conclusion = "–ú–æ–¥–µ–ª—å —Ü–µ–Ω–∏—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ–∫—É–ø–æ–∫ - —Ä–∞–∑–≤–∏–≤–∞–π—Ç–µ –∫—Ä–æ—Å—Å-—Å–µ–ª–ª–∏–Ω–≥"
        elif top_xgb[0][0] == 'recency_days':
            business_conclusion = "–ú–æ–¥–µ–ª—å –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –Ω–µ–¥–∞–≤–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–æ–∫ - —Ñ–æ–∫—É—Å –Ω–∞ —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—é"
        else:
            business_conclusion = "–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∫–ª–∏–µ–Ω—Ç–æ–≤"
        
        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'model_performance': {
                'test_accuracy': safe_float(test_accuracy),
                'total_samples': safe_float(len(X_test))
            },
            'xgboost_feature_importance': {
                'ranking': [(f, safe_float(s)) for f, s in sorted_features],
                'top_3': [(f, safe_float(s)) for f, s in sorted_features[:3]]
            },
            'shap_analysis': {
                'ranking': [(f, safe_float(s)) for f, s in sorted_shap],
                'top_3': [(f, safe_float(s)) for f, s in sorted_shap[:3]],
                'sample_size': len(X_sample),
                'base_value': safe_float(explainer.expected_value)
            },
            'business_insights': {
                'xgb_top_feature': business_insights['xgb_top_feature'],
                'shap_top_feature': business_insights['shap_top_feature'],
                'methods_consistent': business_insights['consistency_check'],
                'business_conclusion': business_conclusion,
                'feature_interpretations': interpretations
            },
            'visualizations': [
                'feature_importance_simple.png',
                'shap_importance_simple.png',
                'shap_summary_simple.png'
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f'interpretability_report_simple_{timestamp}.json'
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # === –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ ===
        
        logger.info("=" * 60)
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –ò–ù–¢–ï–†–ü–†–ï–¢–ò–†–£–ï–ú–û–°–¢–ò")
        logger.info("=" * 60)
        
        logger.info("üèÜ –¢–û–ü-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ (XGBoost):")
        for idx, (feature, importance) in enumerate(top_xgb, 1):
            logger.info(f"   {idx}. {feature}: {importance:.3f}")
        
        logger.info("üèÜ –¢–û–ü-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ (SHAP):")
        for idx, (feature, shap_imp) in enumerate(top_shap, 1):
            logger.info(f"   {idx}. {feature}: {shap_imp:.3f}")
        
        logger.info(f"ü§ù –ú–µ—Ç–æ–¥—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã: {'–î–ê' if business_insights['consistency_check'] else '–ù–ï–¢'}")
        logger.info(f"üíº –ë–∏–∑–Ω–µ—Å-–≤—ã–≤–æ–¥: {business_conclusion}")
        
        logger.info("üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        for viz in report['visualizations']:
            logger.info(f"   - {viz}")
        logger.info(f"   - {report_filename}")
        
        logger.info("‚úÖ –ê–ù–ê–õ–ò–ó –ò–ù–¢–ï–†–ü–†–ï–¢–ò–†–£–ï–ú–û–°–¢–ò –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
