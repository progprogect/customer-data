"""
ML Predictions API Routes
FastAPI routes –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–∫—É–ø–æ–∫

Author: Customer Data Analytics Team
"""

from fastapi import APIRouter, HTTPException, Depends, status, Query
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from typing import List, Optional
import logging
import time
import numpy as np
from datetime import datetime

from models.ml_models import (
    PredictionRequest, PredictionResponse, PredictionResult,
    ThresholdRequest, ModelInfo
)
from services.ml_service import ml_service

logger = logging.getLogger(__name__)

# Security scheme –¥–ª—è API –∫–ª—é—á–µ–π
security = HTTPBearer()

router = APIRouter()


def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
    """
    –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–∞
    –í production —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–µ—Ä—å—ë–∑–Ω—É—é –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é
    """
    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ (–≤ production –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ª–æ–∂–Ω–µ–µ)
    valid_tokens = [
        "dev-token-12345",  # –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        "prod-token-67890"  # –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
    ]
    
    if credentials.credentials not in valid_tokens:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API key",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    return credentials.credentials


@router.post(
    "/purchase-probability",
    response_model=PredictionResponse,
    summary="Predict purchase probability",
    description="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∫—É–ø–∫–∏ –≤ —Å–ª–µ–¥—É—é—â–∏–µ 30 –¥–Ω–µ–π –¥–ª—è batch –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
)
async def predict_purchase_probability(
    request: PredictionRequest,
    api_key: str = Depends(verify_api_key)
) -> PredictionResponse:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∫—É–ø–∫–∏
    
    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        api_key: API –∫–ª—é—á –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
    Returns:
        PredictionResponse: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    start_time = time.time()
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if not ml_service.is_model_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ML model is not loaded. Please contact administrator."
            )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        logger.info(f"üîÆ Purchase probability prediction request: {len(request.rows)} rows")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏
        used_model_version = request.model_version or ml_service.model_version
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
        if request.model_version and request.model_version != ml_service.model_version:
            logger.warning(f"‚ö†Ô∏è Requested model version {request.model_version} != loaded {ml_service.model_version}")
            # –í –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        results = []
        successful_predictions = 0
        failed_predictions = 0
        
        # Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        batch_features = []
        batch_metadata = []
        
        for row in request.rows:
            try:
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                feature_errors = ml_service.validate_features(row.features.dict())
                if feature_errors:
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏
                    results.append(PredictionResult(
                        user_id=row.user_id,
                        snapshot_date=row.snapshot_date,
                        prob_next_30d=None,
                        threshold_applied=False,
                        error=f"Feature validation failed: {'; '.join(feature_errors)}"
                    ))
                    failed_predictions += 1
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ batch
                batch_features.append(row.features.dict())
                batch_metadata.append({
                    'user_id': row.user_id,
                    'snapshot_date': row.snapshot_date
                })
                
            except Exception as e:
                logger.error(f"‚ùå Error processing row for user {row.user_id}: {e}")
                results.append(PredictionResult(
                    user_id=row.user_id,
                    snapshot_date=row.snapshot_date,
                    prob_next_30d=None,
                    threshold_applied=False,
                    error=f"Processing error: {str(e)}"
                ))
                failed_predictions += 1
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        processing_time_ms = 0.0
        
        if batch_features:
            try:
                probabilities, batch_processing_time = ml_service.predict_batch(batch_features)
                processing_time_ms = batch_processing_time
                
                # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for i, (prob, metadata) in enumerate(zip(probabilities, batch_metadata)):
                    results.append(PredictionResult(
                        user_id=metadata['user_id'],
                        snapshot_date=metadata['snapshot_date'],
                        prob_next_30d=float(prob),
                        threshold_applied=False,
                        error=None
                    ))
                    successful_predictions += 1
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É batch
                logger.info(f"üìä Batch prediction stats:")
                logger.info(f"   Size: {len(probabilities)}")
                logger.info(f"   Min prob: {np.min(probabilities):.3f}")
                logger.info(f"   Median prob: {np.median(probabilities):.3f}")
                logger.info(f"   Max prob: {np.max(probabilities):.3f}")
                logger.info(f"   Processing time: {processing_time_ms:.1f}ms")
                
            except Exception as e:
                logger.error(f"‚ùå Batch prediction error: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –≤—Å–µ—Ö –≤ batch
                for metadata in batch_metadata:
                    results.append(PredictionResult(
                        user_id=metadata['user_id'],
                        snapshot_date=metadata['snapshot_date'],
                        prob_next_30d=None,
                        threshold_applied=False,
                        error=f"Batch prediction failed: {str(e)}"
                    ))
                    failed_predictions += 1
        
        # –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        total_time_ms = (time.time() - start_time) * 1000
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = PredictionResponse(
            model_version=used_model_version,
            count=len(request.rows),
            successful_predictions=successful_predictions,
            failed_predictions=failed_predictions,
            processing_time_ms=processing_time_ms,
            results=results
        )
        
        logger.info(f"‚úÖ Prediction completed: {successful_predictions} success, {failed_predictions} failed, {total_time_ms:.1f}ms total")
        
        return response
        
    except HTTPException:
        # –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in prediction: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}"
        )


@router.post(
    "/purchase-probability/apply-threshold",
    response_model=PredictionResponse,
    summary="Predict with threshold",
    description="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –æ —Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–µ"
)
async def predict_with_threshold(
    request: ThresholdRequest,
    api_key: str = Depends(verify_api_key)
) -> PredictionResponse:
    """
    Endpoint –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞
    
    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–æ—Ä–æ–≥–æ–º
        api_key: API –∫–ª—é—á –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
    Returns:
        PredictionResponse: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ñ–ª–∞–≥–æ–º —Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞
    """
    start_time = time.time()
    
    try:
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        logger.info(f"üéØ Threshold prediction request: {len(request.rows)} rows, threshold={request.threshold}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        base_request = PredictionRequest(
            model_version=request.model_version,
            rows=request.rows
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –ø–æ—Ä–æ–≥–∞
        base_response = await predict_purchase_probability(base_request, api_key)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        for result in base_response.results:
            if result.prob_next_30d is not None:
                result.threshold_applied = True
                result.will_target = result.prob_next_30d >= request.threshold
            else:
                result.threshold_applied = False
                result.will_target = None
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è
        total_time_ms = (time.time() - start_time) * 1000
        base_response.processing_time_ms = total_time_ms
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –ø–æ—Ä–æ–≥–æ–º
        successful_results = [r for r in base_response.results if r.prob_next_30d is not None]
        if successful_results:
            targeted_count = sum(1 for r in successful_results if r.will_target)
            targeting_rate = targeted_count / len(successful_results) * 100
            logger.info(f"üéØ Threshold {request.threshold}: {targeted_count}/{len(successful_results)} users targeted ({targeting_rate:.1f}%)")
        
        return base_response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in threshold prediction: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}"
        )


@router.get(
    "/model-info",
    response_model=ModelInfo,
    summary="Get model information",
    description="–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"
)
async def get_model_info(
    api_key: str = Depends(verify_api_key)
) -> ModelInfo:
    """
    Endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
    
    Args:
        api_key: API –∫–ª—é—á –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
    Returns:
        ModelInfo: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    """
    try:
        if not ml_service.is_model_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="ML model is not loaded"
            )
        
        model_info = ml_service.get_model_info()
        
        return ModelInfo(
            model_version=model_info['model_version'],
            load_timestamp=model_info['load_timestamp'],
            feature_names=model_info['feature_names'],
            feature_count=model_info['feature_count'],
            model_performance=model_info['model_performance']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error getting model info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}"
        )


@router.get(
    "/health",
    summary="Health check",
    description="–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è ML —Å–µ—Ä–≤–∏—Å–∞"
)
async def ml_health_check():
    """Health check –¥–ª—è ML —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if not ml_service.is_model_loaded():
            return {
                "status": "unhealthy",
                "reason": "model_not_loaded",
                "timestamp": datetime.now().isoformat()
            }
        
        model_info = ml_service.get_model_info()
        
        return {
            "status": "healthy",
            "model_version": model_info['model_version'],
            "model_loaded": True,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå ML health check error: {e}")
        return {
            "status": "unhealthy",
            "reason": f"error: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
