#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build Training Dataset for XGBoost
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–±–æ—Ä–∫–∏ –µ–¥–∏–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ñ–∏—á –∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤

Author: Customer Data Analytics Team
"""

import psycopg2
import pandas as pd
import logging
import sys
import os
from datetime import datetime
from typing import Dict, Any
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('build_training_dataset.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'dbname': 'customer_data',
    'user': 'mikitavalkunovich',
    'password': '',
    'port': 5432
}

def connect_to_db() -> psycopg2.extensions.connection:
    """
    –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        psycopg2.connection: –û–±—ä–µ–∫—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return conn
    except psycopg2.Error as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def execute_sql_file(conn: psycopg2.extensions.connection, file_path: str) -> bool:
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–∞
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        file_path: –ü—É—Ç—å –∫ SQL —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            sql_content = file.read()
        
        with conn.cursor() as cursor:
            cursor.execute(sql_content)
            conn.commit()
            
        logger.info(f"‚úÖ SQL —Ñ–∞–π–ª {file_path} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL —Ñ–∞–π–ª–∞ {file_path}: {e}")
        conn.rollback()
        return False

def get_dataset_sample(conn: psycopg2.extensions.connection, limit: int = 10) -> pd.DataFrame:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏
        
    Returns:
        pd.DataFrame: –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
    """
    query = f"""
    SELECT 
        user_id,
        snapshot_date,
        recency_days,
        frequency_90d,
        monetary_180d,
        aov_180d,
        orders_lifetime,
        revenue_lifetime,
        categories_unique,
        purchase_next_30d
    FROM ml_training_dataset 
    ORDER BY user_id, snapshot_date 
    LIMIT {limit}
    """
    
    return pd.read_sql_query(query, conn)

def get_feature_statistics(conn: psycopg2.extensions.connection) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        
    Returns:
        Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    """
    query = """
    SELECT 
        -- –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        COUNT(*) as total_rows,
        COUNT(DISTINCT user_id) as unique_users,
        COUNT(DISTINCT snapshot_date) as unique_snapshots,
        
        -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ recency_days
        COUNT(recency_days) as recency_not_null,
        AVG(recency_days) as recency_mean,
        STDDEV(recency_days) as recency_std,
        MIN(recency_days) as recency_min,
        MAX(recency_days) as recency_max,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY recency_days) as recency_median,
        
        -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ frequency_90d
        AVG(frequency_90d) as frequency_mean,
        STDDEV(frequency_90d) as frequency_std,
        MIN(frequency_90d) as frequency_min,
        MAX(frequency_90d) as frequency_max,
        
        -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ monetary_180d
        AVG(monetary_180d) as monetary_mean,
        STDDEV(monetary_180d) as monetary_std,
        MIN(monetary_180d) as monetary_min,
        MAX(monetary_180d) as monetary_max,
        
        -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ orders_lifetime
        AVG(orders_lifetime) as orders_mean,
        STDDEV(orders_lifetime) as orders_std,
        MIN(orders_lifetime) as orders_min,
        MAX(orders_lifetime) as orders_max,
        
        -- –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        COUNT(CASE WHEN purchase_next_30d = TRUE THEN 1 END) as positive_class,
        COUNT(CASE WHEN purchase_next_30d = FALSE THEN 1 END) as negative_class
        
    FROM ml_training_dataset
    """
    
    with conn.cursor() as cursor:
        cursor.execute(query)
        result = cursor.fetchone()
        
        if result:
            columns = [desc[0] for desc in cursor.description]
            return dict(zip(columns, result))
    
    return {}

def export_dataset_to_csv(conn: psycopg2.extensions.connection, output_path: str) -> bool:
    """
    –≠–∫—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ CSV —Ñ–∞–π–ª
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω
    """
    try:
        query = """
        SELECT 
            user_id,
            snapshot_date,
            recency_days,
            frequency_90d,
            monetary_180d,
            aov_180d,
            orders_lifetime,
            revenue_lifetime,
            categories_unique,
            purchase_next_30d::int as target
        FROM ml_training_dataset 
        ORDER BY user_id, snapshot_date
        """
        
        df = pd.read_sql_query(query, conn)
        df.to_csv(output_path, index=False)
        
        logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ CSV: {output_path}")
        logger.info(f"üìä –†–∞–∑–º–µ—Ä —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df):,} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∫–∏ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è XGBoost")
    
    # –ü—É—Ç—å –∫ SQL —Ñ–∞–π–ª–∞–º
    sql_dir = os.path.join(os.path.dirname(__file__), '..', 'sql')
    
    sql_files = [
        'create_training_dataset_table.sql',  # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        'populate_training_dataset.sql',      # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        'validate_training_dataset.sql'       # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
    ]
    
    conn = None
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        conn = connect_to_db()
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for sql_file in sql_files:
            file_path = os.path.join(sql_dir, sql_file)
            
            if not os.path.exists(file_path):
                logger.error(f"‚ùå SQL —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                continue
                
            logger.info(f"üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {sql_file}...")
            success = execute_sql_file(conn, file_path)
            
            if not success:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {sql_file}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                return False
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        logger.info("üìä –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º...")
        feature_stats = get_feature_statistics(conn)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–∞ –¥–∞–Ω–Ω—ã—Ö...")
        sample_data = get_dataset_sample(conn, limit=5)
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
        csv_path = 'ml_training_dataset.csv'
        logger.info(f"üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ CSV: {csv_path}")
        export_success = export_dataset_to_csv(conn, csv_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        report = {
            'generation_time': datetime.now().isoformat(),
            'feature_statistics': feature_stats,
            'sample_data': sample_data.to_dict('records'),
            'csv_export_success': export_success,
            'csv_file_path': csv_path if export_success else None
        }
        
        with open('training_dataset_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str, ensure_ascii=False)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        logger.info("=" * 60)
        logger.info("üéâ –°–ë–û–†–ö–ê –û–ë–£–ß–ê–Æ–©–ï–ì–û –î–ê–¢–ê–°–ï–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        logger.info("=" * 60)
        
        if feature_stats:
            logger.info(f"üìä –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {feature_stats.get('total_rows', 0):,}")
            logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {feature_stats.get('unique_users', 0):,}")
            logger.info(f"  ‚Ä¢ –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {feature_stats.get('positive_class', 0):,}")
            logger.info(f"  ‚Ä¢ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {feature_stats.get('negative_class', 0):,}")
            
            total = feature_stats.get('total_rows', 0)
            positive = feature_stats.get('positive_class', 0)
            if total > 0:
                positive_percent = (positive / total) * 100
                logger.info(f"  ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞: {positive_percent:.2f}%")
        
        logger.info(f"üìÅ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: training_dataset_report.json")
        if export_success:
            logger.info(f"üìÅ CSV —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {csv_path}")
        
        logger.info("‚úÖ –î–ê–¢–ê–°–ï–¢ –ì–û–¢–û–í –ö –û–ë–£–ß–ï–ù–ò–Æ –ú–û–î–ï–õ–ò!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False
        
    finally:
        if conn:
            conn.close()
            logger.info("üîê –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
