#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Model Interpretability Analysis
–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –¥–ª—è XGBoost –º–æ–¥–µ–ª–∏

Features:
- Feature importance visualization
- SHAP global and local explanations
- Business insights generation

Author: Customer Data Analytics Team
"""

import pandas as pd
import numpy as np
import joblib
import json
import warnings
warnings.filterwarnings('ignore')

# ML libraries
import xgboost as xgb
from sklearn.metrics import classification_report
import shap

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# System libraries
import logging
import sys
import os
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è matplotlib –Ω–∞ —Å–µ—Ä–≤–µ—Ä–∞—Ö –±–µ–∑ GUI
plt.switch_backend('Agg')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('interpretability_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class ModelInterpreter:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å)
        """
        self.model = None
        self.scaler = None
        self.feature_names = [
            'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
            'orders_lifetime', 'revenue_lifetime', 'categories_unique'
        ]
        self.explainer = None
        self.shap_values = None
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        
        logger.info("üîç Model Interpreter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def train_simple_model(self) -> None:
        """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π XGBoost –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_df = pd.read_csv('train_set.csv')
        test_df = pd.read_csv('test_set.csv')
        
        X_train = train_df[self.feature_names].fillna(train_df[self.feature_names].median())
        y_train = train_df['target']
        
        X_test = test_df[self.feature_names].fillna(train_df[self.feature_names].median())
        y_test = test_df['target']
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å —Ö–æ—Ä–æ—à–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            scale_pos_weight=1.57
        )
        
        self.model.fit(X_train, y_train)
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞
        y_pred = self.model.predict(X_test)
        accuracy = (y_pred == y_test).mean()
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. Test accuracy: {accuracy:.3f}")
    
    def load_model(self, model_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.model = joblib.load(model_path)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def analyze_feature_importance(self) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üìä –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ train_simple_model() –∏–ª–∏ load_model()")
        
        # XGBoost feature importance
        importance_gain = self.model.feature_importances_
        importance_dict = dict(zip(self.feature_names, importance_gain))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        importance_df = pd.DataFrame(sorted_features, columns=['feature', 'importance'])
        importance_df['importance_percent'] = importance_df['importance'] / importance_df['importance'].sum() * 100
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ø-—Ñ–∏—á–µ–π
        logger.info("üèÜ –¢–û–ü-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for idx, (feature, importance) in enumerate(sorted_features[:5], 1):
            logger.info(f"   {idx}. {feature}: {importance:.3f} ({importance/sum(importance_gain)*100:.1f}%)")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        plt.figure(figsize=(10, 6))
        sns.barplot(data=importance_df, x='importance_percent', y='feature', palette='viridis')
        plt.title('Feature Importance (XGBoost)', fontsize=14, fontweight='bold')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å (%)', fontsize=12)
        plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏', fontsize=12)
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: feature_importance.png")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        results = {
            'feature_importance': importance_dict,
            'sorted_features': sorted_features,
            'top_3_features': sorted_features[:3],
            'interpretation': self._interpret_feature_importance(sorted_features)
        }
        
        return results
    
    def _interpret_feature_importance(self, sorted_features: list) -> dict:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        interpretations = {
            'recency_days': "–î–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏ - –∫–ª—é—á–µ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –ø–æ–∫—É–ø–∫–µ",
            'frequency_90d': "–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫ –∑–∞ 90 –¥–Ω–µ–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞", 
            'monetary_180d': "–ü–æ—Ç—Ä–∞—á–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–∞ 180 –¥–Ω–µ–π - –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏",
            'aov_180d': "–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –∑–∞ 180 –¥–Ω–µ–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞",
            'orders_lifetime': "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤ - –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
            'revenue_lifetime': "–û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å lifetime value",
            'categories_unique': "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π - –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø–æ–∫—É–ø–æ–∫"
        }
        
        top_feature = sorted_features[0][0]
        top_importance = sorted_features[0][1]
        
        business_insights = {
            'most_important_feature': top_feature,
            'most_important_interpretation': interpretations.get(top_feature, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ñ–∏—á–∞"),
            'dominance_score': top_importance / sum([f[1] for f in sorted_features]),
            'business_conclusion': self._generate_business_conclusion(sorted_features[:3])
        }
        
        return business_insights
    
    def _generate_business_conclusion(self, top_features: list) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–≤—ã–≤–æ–¥–æ–≤"""
        if top_features[0][0] == 'recency_days':
            return "–ú–æ–¥–µ–ª—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –Ω–µ–¥–∞–≤–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–æ–∫ - —Ñ–æ–∫—É—Å –Ω–∞ —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—é"
        elif top_features[0][0] in ['frequency_90d', 'orders_lifetime']:
            return "–ú–æ–¥–µ–ª—å –±–æ–ª—å—à–µ —Ü–µ–Ω–∏—Ç —á–∞—Å—Ç–æ—Ç—É –ø–æ–∫—É–ø–æ–∫ - —Ñ–æ–∫—É—Å –Ω–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"
        elif top_features[0][0] in ['monetary_180d', 'revenue_lifetime', 'aov_180d']:
            return "–ú–æ–¥–µ–ª—å –∞–∫—Ü–µ–Ω—Ç–∏—Ä—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –¥–µ–Ω–µ–∂–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ - —Ñ–æ–∫—É—Å –Ω–∞ VIP-–∫–ª–∏–µ–Ω—Ç–æ–≤"
        else:
            return "–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∫–ª–∏–µ–Ω—Ç–æ–≤"
    
    def calculate_shap_values(self, sample_size: int = 1000) -> dict:
        """–†–∞—Å—á–µ—Ç SHAP –∑–Ω–∞—á–µ–Ω–∏–π"""
        logger.info(f"üî¨ –†–∞—Å—á–µ—Ç SHAP –∑–Ω–∞—á–µ–Ω–∏–π (sample_size={sample_size})...")
        
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_df = pd.read_csv('test_set.csv')
        X_test = test_df[self.feature_names].fillna(test_df[self.feature_names].median())
        
        # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è SHAP
        if len(X_test) > sample_size:
            X_sample = X_test.sample(n=sample_size, random_state=42)
        else:
            X_sample = X_test
        
        # –°–æ–∑–¥–∞–Ω–∏–µ SHAP explainer
        logger.info("üßÆ –°–æ–∑–¥–∞–Ω–∏–µ SHAP explainer...")
        self.explainer = shap.TreeExplainer(self.model)
        
        # –†–∞—Å—á–µ—Ç SHAP –∑–Ω–∞—á–µ–Ω–∏–π
        logger.info("‚ö° –†–∞—Å—á–µ—Ç SHAP –∑–Ω–∞—á–µ–Ω–∏–π...")
        self.shap_values = self.explainer.shap_values(X_sample)
        
        # Summary plot
        logger.info("üìà –°–æ–∑–¥–∞–Ω–∏–µ SHAP summary plot...")
        plt.figure(figsize=(10, 6))
        shap.summary_plot(self.shap_values, X_sample, feature_names=self.feature_names, show=False)
        plt.tight_layout()
        plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Feature importance from SHAP
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ SHAP feature importance...")
        plt.figure(figsize=(8, 6))
        shap.summary_plot(self.shap_values, X_sample, feature_names=self.feature_names, 
                         plot_type="bar", show=False)
        plt.tight_layout()
        plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Waterfall plot –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        logger.info("üíß –°–æ–∑–¥–∞–Ω–∏–µ SHAP waterfall plot...")
        try:
            plt.figure(figsize=(10, 6))
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Explanation –¥–ª—è waterfall plot
            explanation = shap.Explanation(
                values=self.shap_values[0], 
                base_values=self.explainer.expected_value,
                data=X_sample.iloc[0].values,
                feature_names=self.feature_names
            )
            shap.plots.waterfall(explanation, show=False)
            plt.tight_layout()
            plt.savefig('shap_waterfall_example.png', dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å waterfall plot: {e}")
            # –°–æ–∑–¥–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
            plt.figure(figsize=(10, 6))
            feature_values = X_sample.iloc[0].values
            shap_vals = self.shap_values[0]
            
            # Simple bar plot as alternative
            y_pos = np.arange(len(self.feature_names))
            plt.barh(y_pos, shap_vals)
            plt.yticks(y_pos, self.feature_names)
            plt.xlabel('SHAP –∑–Ω–∞—á–µ–Ω–∏–µ')
            plt.title(f'SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ 1\n(base_value: {self.explainer.expected_value:.3f})')
            plt.tight_layout()
            plt.savefig('shap_waterfall_example.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ SHAP –∑–Ω–∞—á–µ–Ω–∏–π
        shap_stats = {
            'mean_abs_shap': [float(x) for x in np.mean(np.abs(self.shap_values), axis=0)],
            'feature_names': self.feature_names,
            'base_value': float(self.explainer.expected_value),
            'sample_size': int(len(X_sample))
        }
        
        # –¢–æ–ø —Ñ–∏—á–∏ –ø–æ SHAP
        feature_shap_importance = dict(zip(self.feature_names, shap_stats['mean_abs_shap']))
        sorted_shap_features = sorted(feature_shap_importance.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("üèÜ –¢–û–ü-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ SHAP –≤–∞–∂–Ω–æ—Å—Ç–∏:")
        for idx, (feature, shap_imp) in enumerate(sorted_shap_features[:5], 1):
            logger.info(f"   {idx}. {feature}: {shap_imp:.3f}")
        
        logger.info("üìä SHAP –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        logger.info("   - shap_summary.png")
        logger.info("   - shap_feature_importance.png")
        logger.info("   - shap_waterfall_example.png")
        
        results = {
            'shap_statistics': shap_stats,
            'shap_feature_ranking': sorted_shap_features,
            'interpretation': self._interpret_shap_results(sorted_shap_features)
        }
        
        return results
    
    def _interpret_shap_results(self, sorted_shap_features: list) -> dict:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è SHAP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        shap_interpretations = {
            'top_shap_feature': sorted_shap_features[0][0],
            'shap_vs_xgb_consistency': self._check_feature_ranking_consistency(sorted_shap_features),
            'local_explanation_available': True,
            'business_recommendation': self._generate_shap_business_recommendation(sorted_shap_features[:3])
        }
        
        return shap_interpretations
    
    def _check_feature_ranking_consistency(self, shap_ranking: list) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É XGBoost –∏ SHAP ranking"""
        # –≠—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å XGBoost importance
        return "SHAP ranking —Ä–∞—Å—Å—á–∏—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ"
    
    def _generate_shap_business_recommendation(self, top_shap_features: list) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SHAP"""
        top_feature = top_shap_features[0][0]
        
        recommendations = {
            'recency_days': "–§–æ–∫—É—Å –Ω–∞ —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
            'frequency_90d': "–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞—Å—Ç—ã—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
            'monetary_180d': "–†–∞–±–æ—Ç–∞ —Å –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–µ–º–∏—É–º-—Å–µ—Ä–≤–∏—Å",
            'aov_180d': "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞ —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-—Å–µ–ª–ª–∏–Ω–≥",
            'orders_lifetime': "–†–∞–∑–≤–∏—Ç–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Å –ª–æ—è–ª—å–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏",
            'revenue_lifetime': "VIP-–ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º LTV",
            'categories_unique': "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –ø–æ–∫—É–ø–∫–∞–º–∏"
        }
        
        return recommendations.get(top_feature, "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    def generate_comprehensive_report(self) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏...")
        
        # –ê–Ω–∞–ª–∏–∑ feature importance
        fi_results = self.analyze_feature_importance()
        
        # SHAP –∞–Ω–∞–ª–∏–∑
        shap_results = self.calculate_shap_values(sample_size=500)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        comprehensive_report = {
            'timestamp': datetime.now().isoformat(),
            'model_type': 'XGBoost',
            'analysis_type': 'Feature Importance + SHAP',
            'feature_importance_analysis': fi_results,
            'shap_analysis': shap_results,
            'summary': {
                'most_important_xgb': fi_results['top_3_features'],
                'most_important_shap': shap_results['shap_feature_ranking'][:3],
                'business_insights': {
                    'xgb_conclusion': fi_results['interpretation']['business_conclusion'],
                    'shap_recommendation': shap_results['interpretation']['business_recommendation']
                }
            },
            'visualizations_created': [
                'feature_importance.png',
                'shap_summary.png', 
                'shap_feature_importance.png',
                'shap_waterfall_example.png'
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f'interpretability_report_{timestamp}.json'
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÅ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")
        
        return comprehensive_report

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("=" * 60)
    logger.info("üîç –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –ò–ù–¢–ï–†–ü–†–ï–¢–ò–†–£–ï–ú–û–°–¢–ò –ú–û–î–ï–õ–ò")
    logger.info("=" * 60)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞
        interpreter = ModelInterpreter()
        
        # –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        interpreter.train_simple_model()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report = interpreter.generate_comprehensive_report()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("=" * 60)
        logger.info("üìä –ö–õ–Æ–ß–ï–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        logger.info("=" * 60)
        
        # XGBoost feature importance
        xgb_top3 = report['feature_importance_analysis']['top_3_features']
        logger.info("üèÜ –¢–û–ü-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ (XGBoost importance):")
        for idx, (feature, importance) in enumerate(xgb_top3, 1):
            logger.info(f"   {idx}. {feature}: {importance:.3f}")
        
        # SHAP feature importance
        shap_top3 = report['shap_analysis']['shap_feature_ranking'][:3]
        logger.info("üèÜ –¢–û–ü-3 –ø—Ä–∏–∑–Ω–∞–∫–∞ (SHAP importance):")
        for idx, (feature, shap_imp) in enumerate(shap_top3, 1):
            logger.info(f"   {idx}. {feature}: {shap_imp:.3f}")
        
        # –ë–∏–∑–Ω–µ—Å-–≤—ã–≤–æ–¥—ã
        logger.info("üíº –ë–ò–ó–ù–ï–°-–í–´–í–û–î–´:")
        logger.info(f"   XGBoost: {report['summary']['business_insights']['xgb_conclusion']}")
        logger.info(f"   SHAP: {report['summary']['business_insights']['shap_recommendation']}")
        
        # –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        logger.info("üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        for file in report['visualizations_created']:
            logger.info(f"   - {file}")
        
        logger.info("‚úÖ –ê–ù–ê–õ–ò–ó –ò–ù–¢–ï–†–ü–†–ï–¢–ò–†–£–ï–ú–û–°–¢–ò –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
