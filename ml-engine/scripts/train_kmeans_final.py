#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Train final K-means model and save results to database
Author: Customer Data Analytics Team
"""

import numpy as np
import pandas as pd
import psycopg2
from sklearn.cluster import KMeans
import logging
from typing import Tuple, Dict
import sys
import os
import json
from datetime import date

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from prepare_data_for_clustering import main as prepare_data

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'dbname': 'customer_data',
    'user': 'mikitavalkunovich',
    'password': '',
    'port': 5432
}

def connect_to_db() -> psycopg2.extensions.connection:
    """
    –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        psycopg2.connection: –û–±—ä–µ–∫—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return conn
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def create_segments_table(conn: psycopg2.extensions.connection) -> None:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã user_segments_kmeans
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    """
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS user_segments_kmeans (
        user_id BIGINT NOT NULL,
        snapshot_date DATE NOT NULL,
        cluster_id INT NOT NULL,
        created_at TIMESTAMPTZ DEFAULT NOW(),
        PRIMARY KEY (user_id, snapshot_date)
    );
    
    -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    CREATE INDEX IF NOT EXISTS idx_user_segments_user_id ON user_segments_kmeans(user_id);
    CREATE INDEX IF NOT EXISTS idx_user_segments_cluster ON user_segments_kmeans(cluster_id);
    CREATE INDEX IF NOT EXISTS idx_user_segments_date ON user_segments_kmeans(snapshot_date);
    
    -- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ —Ç–∞–±–ª–∏—Ü–µ
    COMMENT ON TABLE user_segments_kmeans IS '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π';
    COMMENT ON COLUMN user_segments_kmeans.user_id IS 'ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è';
    COMMENT ON COLUMN user_segments_kmeans.snapshot_date IS '–î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏';
    COMMENT ON COLUMN user_segments_kmeans.cluster_id IS 'ID –∫–ª–∞—Å—Ç–µ—Ä–∞ (0, 1, 2, ...)';
    """
    
    try:
        cur = conn.cursor()
        cur.execute(create_table_sql)
        conn.commit()
        cur.close()
        logger.info("–¢–∞–±–ª–∏—Ü–∞ user_segments_kmeans —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        raise

def train_kmeans_model(X_scaled: np.ndarray, k: int = 3) -> Tuple[KMeans, np.ndarray]:
    """
    –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ K-means
    
    Args:
        X_scaled: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        
    Returns:
        Tuple[KMeans, np.ndarray]: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
    """
    logger.info(f"–û–±—É—á–µ–Ω–∏–µ K-means –º–æ–¥–µ–ª–∏ —Å k = {k}")
    
    kmeans = KMeans(
        n_clusters=k,
        random_state=42,
        n_init=10,
        max_iter=300
    )
    
    labels = kmeans.fit_predict(X_scaled)
    
    logger.info(f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {k}")
    logger.info(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X_scaled.shape}")
    logger.info(f"Inertia: {kmeans.inertia_:.2f}")
    
    return kmeans, labels

def save_clustering_results(conn: psycopg2.extensions.connection, 
                          user_ids: pd.Series, 
                          cluster_labels: np.ndarray,
                          snapshot_date: date = None) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ë–î
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        user_ids: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        cluster_labels: –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        snapshot_date: –î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–µ–≥–æ–¥–Ω—è)
    """
    if snapshot_date is None:
        snapshot_date = date.today()
    
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    
    try:
        cur = conn.cursor()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        data_to_insert = []
        for user_id, cluster_id in zip(user_ids, cluster_labels):
            data_to_insert.append((int(user_id), snapshot_date, int(cluster_id)))
        
        # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        insert_sql = """
        INSERT INTO user_segments_kmeans (user_id, snapshot_date, cluster_id)
        VALUES (%s, %s, %s)
        ON CONFLICT (user_id, snapshot_date) DO UPDATE SET
            cluster_id = EXCLUDED.cluster_id,
            created_at = NOW();
        """
        
        cur.executemany(insert_sql, data_to_insert)
        conn.commit()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        cur.execute("""
            SELECT cluster_id, COUNT(*) as count 
            FROM user_segments_kmeans 
            WHERE snapshot_date = %s 
            GROUP BY cluster_id 
            ORDER BY cluster_id
        """, (snapshot_date,))
        
        cluster_counts = cur.fetchall()
        
        logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î:")
        for cluster_id, count in cluster_counts:
            logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        cur.close()
        
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        raise

def analyze_clusters(conn: psycopg2.extensions.connection, 
                    snapshot_date: date = None) -> pd.DataFrame:
    """
    –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º RFM-–ø—Ä–∏–∑–Ω–∞–∫–∞–º
    
    Args:
        conn: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        snapshot_date: –î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞
        
    Returns:
        pd.DataFrame: –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """
    if snapshot_date is None:
        snapshot_date = date.today()
    
    logger.info("–ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    analysis_sql = """
    WITH cluster_analysis AS (
        SELECT 
            s.cluster_id,
            COUNT(*) as users_count,
            AVG(u.recency_days) as avg_recency_days,
            AVG(u.frequency_90d) as avg_frequency_90d,
            AVG(u.monetary_180d) as avg_monetary_180d,
            AVG(u.aov_180d) as avg_aov_180d,
            AVG(u.orders_lifetime) as avg_orders_lifetime,
            AVG(u.revenue_lifetime) as avg_revenue_lifetime,
            AVG(u.categories_unique) as avg_categories_unique,
            MIN(u.recency_days) as min_recency_days,
            MAX(u.recency_days) as max_recency_days,
            MIN(u.monetary_180d) as min_monetary_180d,
            MAX(u.monetary_180d) as max_monetary_180d
        FROM user_segments_kmeans s
        JOIN ml_user_features_daily_buyers u ON u.user_id = s.user_id 
            AND u.snapshot_date = s.snapshot_date
        WHERE s.snapshot_date = %s
        GROUP BY s.cluster_id
        ORDER BY s.cluster_id
    )
    SELECT * FROM cluster_analysis;
    """
    
    try:
        df_analysis = pd.read_sql(analysis_sql, conn, params=(snapshot_date,))
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω –¥–ª—è {len(df_analysis)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return df_analysis
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
        raise

def interpret_clusters(df_analysis: pd.DataFrame) -> Dict[int, str]:
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    
    Args:
        df_analysis: DataFrame —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        
    Returns:
        Dict[int, str]: –°–ª–æ–≤–∞—Ä—å cluster_id -> –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
    """
    interpretations = {}
    
    for _, row in df_analysis.iterrows():
        cluster_id = int(row['cluster_id'])
        recency = row['avg_recency_days']
        frequency = row['avg_frequency_90d']
        monetary = row['avg_monetary_180d']
        aov = row['avg_aov_180d']
        categories = row['avg_categories_unique']
        
        # –õ–æ–≥–∏–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ RFM-–∞–Ω–∞–ª–∏–∑–∞
        if recency <= 30 and frequency >= 4 and monetary >= 2000:
            interpretation = "VIP / –õ–æ—è–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
        elif recency <= 60 and frequency >= 2 and monetary >= 500:
            interpretation = "–ê–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
        elif recency <= 120 and frequency >= 1 and monetary >= 100:
            interpretation = "–û–±—ã—á–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
        elif recency > 120 and frequency < 2 and monetary < 200:
            interpretation = "–°–ø—è—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã"
        elif monetary >= 1000:
            interpretation = "–í—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
        elif frequency >= 3:
            interpretation = "–ß–∞—Å—Ç—ã–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª–∏"
        else:
            interpretation = "–ù–∏–∑–∫–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
        
        interpretations[cluster_id] = interpretation
    
    return interpretations

def print_cluster_analysis(df_analysis: pd.DataFrame, interpretations: Dict[int, str]) -> None:
    """
    –í—ã–≤–æ–¥ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    
    Args:
        df_analysis: DataFrame —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        interpretations: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è–º–∏
    """
    print("\n" + "="*80)
    print("–ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô")
    print("="*80)
    
    for _, row in df_analysis.iterrows():
        cluster_id = int(row['cluster_id'])
        interpretation = interpretations.get(cluster_id, "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
        
        print(f"\nüîπ –ö–õ–ê–°–¢–ï–† {cluster_id}: {interpretation}")
        print("-" * 60)
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {int(row['users_count'])}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞: {row['users_count']/df_analysis['users_count'].sum()*100:.1f}%")
        
        print(f"\nüìä RFM-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
        print(f"  Recency (–¥–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏): {row['avg_recency_days']:.1f} (–¥–∏–∞–ø–∞–∑–æ–Ω: {row['min_recency_days']:.0f}-{row['max_recency_days']:.0f})")
        print(f"  Frequency (–∑–∞–∫–∞–∑—ã –∑–∞ 90 –¥–Ω–µ–π): {row['avg_frequency_90d']:.1f}")
        print(f"  Monetary (—Å—É–º–º–∞ –∑–∞ 180 –¥–Ω–µ–π): ${row['avg_monetary_180d']:.2f} (–¥–∏–∞–ø–∞–∑–æ–Ω: ${row['min_monetary_180d']:.2f}-${row['max_monetary_180d']:.2f})")
        print(f"  AOV (—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫): ${row['avg_aov_180d']:.2f}")
        print(f"  –ó–∞–∫–∞–∑–æ–≤ –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è: {row['avg_orders_lifetime']:.1f}")
        print(f"  –í—ã—Ä—É—á–∫–∞ –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è: ${row['avg_revenue_lifetime']:.2f}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {row['avg_categories_unique']:.1f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç—É
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        if "VIP" in interpretation or "–õ–æ—è–ª—å–Ω—ã–µ" in interpretation:
            print("  - –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –∞–∫—Ü–∏–∏")
            print("  - –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–≤—ã—à–µ–Ω–Ω—ã–º–∏ –±–æ–Ω—É—Å–∞–º–∏")
            print("  - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞")
        elif "–ê–∫—Ç–∏–≤–Ω—ã–µ" in interpretation:
            print("  - –†–µ–≥—É–ª—è—Ä–Ω—ã–µ email-—Ä–∞—Å—Å—ã–ª–∫–∏ —Å –Ω–æ–≤–∏–Ω–∫–∞–º–∏")
            print("  - –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏")
            print("  - –ö—Ä–æ—Å—Å-–ø—Ä–æ–¥–∞–∂–∏ –∏ –∞–ø—Å–µ–ª–ª—ã")
        elif "–û–±—ã—á–Ω—ã–µ" in interpretation:
            print("  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏")
            print("  - –°–∫–∏–¥–∫–∏ –∏ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–∏")
            print("  - –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã –ø–æ–∫—É–ø–æ–∫")
        elif "–°–ø—è—â–∏–µ" in interpretation:
            print("  - –†–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏")
            print("  - –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞")
            print("  - –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω —É—Ö–æ–¥–∞")
        elif "–í—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã–µ" in interpretation:
            print("  - –ü—Ä–µ–º–∏—É–º-–ø—Ä–æ–¥—É–∫—Ç—ã –∏ —É—Å–ª—É–≥–∏")
            print("  - –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã")
            print("  - –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        elif "–ß–∞—Å—Ç—ã–µ" in interpretation:
            print("  - –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–¥–ø–∏—Å–æ–∫")
            print("  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–∞–∑—ã")
            print("  - –ë–æ–Ω—É—Å—ã –∑–∞ —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å")

def save_analysis_results(df_analysis: pd.DataFrame, interpretations: Dict[int, str], 
                         snapshot_date: date = None) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª—ã
    
    Args:
        df_analysis: DataFrame —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        interpretations: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è–º–∏
        snapshot_date: –î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞
    """
    if snapshot_date is None:
        snapshot_date = date.today()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    csv_path = f'ml-engine/results/cluster_analysis_{snapshot_date}.csv'
    df_analysis.to_csv(csv_path, index=False)
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {csv_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π –≤ JSON
    json_path = f'ml-engine/results/cluster_interpretations_{snapshot_date}.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(interpretations, f, indent=2, ensure_ascii=False)
    logger.info(f"–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {json_path}")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è K-means –∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ K-means")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_scaled, user_ids, scaler = prepare_data()
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    conn = connect_to_db()
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        create_segments_table(conn)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        kmeans, cluster_labels = train_kmeans_model(X_scaled, k=3)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_clustering_results(conn, user_ids, cluster_labels)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        df_analysis = analyze_clusters(conn)
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        interpretations = interpret_clusters(df_analysis)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print_cluster_analysis(df_analysis, interpretations)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        save_analysis_results(df_analysis, interpretations)
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ K-means –∏ –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        
        return kmeans, cluster_labels, df_analysis, interpretations
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {e}")
        raise
    finally:
        if conn:
            conn.close()
            logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")

if __name__ == "__main__":
    kmeans, labels, analysis, interpretations = main()
