"""
ML Service for Purchase Probability Predictions
–°–µ—Ä–≤–∏—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è inference

Author: Customer Data Analytics Team
"""

import joblib
import json
import pandas as pd
import numpy as np
import logging
import os
import time
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, date
from pathlib import Path

logger = logging.getLogger(__name__)


class MLModelService:
    """Singleton —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ML –º–æ–¥–µ–ª—å—é"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MLModelService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            # Purchase prediction model
            self.model = None
            self.scaler = None
            self.metadata = None
            self.model_version = None
            self.feature_names = None
            self.fill_values = None
            self.load_timestamp = None
            
            # Churn prediction model
            self.churn_model = None
            self.churn_model_version = None
            self.churn_load_timestamp = None
            self.churn_feature_importance = None
            
            self._initialized = True
            logger.info("ü§ñ ML Model Service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        
        Args:
            model_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –º–æ–¥–µ–ª—å—é (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é)
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏
        """
        try:
            if model_path is None:
                model_path = self._find_latest_model()
            
            if not model_path:
                raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é")
            
            logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}")
            
            # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
            model_file = Path(model_path) / "xgboost_model.pkl"
            scaler_file = Path(model_path) / "scaler.pkl"
            metadata_file = Path(model_path) / "model_metadata.json"
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            for file_path in [model_file, scaler_file, metadata_file]:
                if not file_path.exists():
                    raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ XGBoost –º–æ–¥–µ–ª–∏...")
            self.model = joblib.load(model_file)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫–µ–π–ª–µ—Ä–∞
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ StandardScaler...")
            self.scaler = joblib.load(scaler_file)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
            with open(metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            self.model_version = self.metadata.get('model_version', 'unknown')
            self.feature_names = self.metadata.get('feature_names', [])
            self.fill_values = self.metadata.get('fill_values', {})
            self.load_timestamp = datetime.now().isoformat()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            self._validate_loaded_model()
            
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            logger.info(f"   üìå –í–µ—Ä—Å–∏—è: {self.model_version}")
            logger.info(f"   üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names)}")
            logger.info(f"   üïê –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {self.load_timestamp}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._reset_model()
            return False
    
    def _find_latest_model(self) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏"""
        try:
            # –ò—â–µ–º –≤ ml-engine/scripts –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            base_path = Path(__file__).parent.parent.parent / "ml-engine" / "scripts"
            
            if not base_path.exists():
                logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_path}")
                return None
            
            # –ò—â–µ–º –ø–∞–ø–∫–∏ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º production_model_*
            model_dirs = list(base_path.glob("production_model_*"))
            
            if not model_dirs:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏")
                return None
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ (–≤–µ—Ä—Å–∏–∏) –∏ –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é
            latest_model = sorted(model_dirs, reverse=True)[0]
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å: {latest_model}")
            
            return str(latest_model)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def _validate_loaded_model(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        if self.scaler is None:
            raise ValueError("–°–∫–µ–π–ª–µ—Ä –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        if not self.feature_names:
            raise ValueError("–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        if not self.fill_values:
            raise ValueError("–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è NaN")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        if not hasattr(self.model, 'predict_proba'):
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
        
        if not hasattr(self.scaler, 'transform'):
            raise ValueError("–°–∫–µ–π–ª–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç transform")
        
        logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ")
    
    def _reset_model(self):
        """–°–±—Ä–æ—Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        self.model = None
        self.scaler = None
        self.metadata = None
        self.model_version = None
        self.feature_names = None
        self.fill_values = None
        self.load_timestamp = None
    
    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self.model is not None and self.scaler is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.is_model_loaded():
            return {"status": "not_loaded"}
        
        return {
            "status": "loaded",
            "model_version": self.model_version,
            "load_timestamp": self.load_timestamp,
            "feature_names": self.feature_names,
            "feature_count": len(self.feature_names),
            "model_performance": self.metadata.get('test_results', {}).get('metrics', {}),
            "fill_values": self.fill_values
        }
    
    def prepare_features(self, features_list: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è inference
        
        Args:
            features_list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            pd.DataFrame: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        if not self.is_model_loaded():
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(features_list)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN
        for feature_name in self.feature_names:
            if feature_name not in df.columns:
                df[feature_name] = self.fill_values.get(feature_name, 0.0)
            else:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ –æ–±—É—á–µ–Ω–∏—è
                fill_value = self.fill_values.get(feature_name, 0.0)
                df[feature_name] = df[feature_name].fillna(fill_value)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –∫–æ–ª–æ–Ω–æ–∫
        df_ordered = df[self.feature_names].copy()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        df_scaled = pd.DataFrame(
            self.scaler.transform(df_ordered),
            columns=self.feature_names,
            index=df_ordered.index
        )
        
        return df_scaled
    
    def predict_probabilities(self, prepared_features: pd.DataFrame) -> np.ndarray:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ inference
        
        Args:
            prepared_features: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            
        Returns:
            np.ndarray: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1
        """
        if not self.is_model_loaded():
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        probabilities = self.model.predict_proba(prepared_features)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1 (purchase = True)
        return probabilities[:, 1]
    
    def predict_batch(self, features_list: List[Dict[str, Any]]) -> Tuple[np.ndarray, float]:
        """
        Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
        
        Args:
            features_list: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            Tuple[np.ndarray, float]: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –º—Å
        """
        start_time = time.time()
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            prepared_features = self.prepare_features(features_list)
            
            # Inference
            probabilities = self.predict_probabilities(prepared_features)
            
            # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processing_time_ms = (time.time() - start_time) * 1000
            
            return probabilities, processing_time_ms
            
        except Exception as e:
            processing_time_ms = (time.time() - start_time) * 1000
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            raise
    
    def validate_features(self, features: Dict[str, Any]) -> List[str]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        errors = []
        
        for feature_name in self.feature_names:
            if feature_name in features:
                value = features[feature_name]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–µ –∏–ª–∏ None
                if value is not None and not isinstance(value, (int, float)):
                    errors.append(f"Feature '{feature_name}' must be numeric, got {type(value).__name__}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ñ–∏—á–µ–π
                if value is not None and value < 0:
                    if feature_name in ['frequency_90d', 'monetary_180d', 'aov_180d', 
                                      'orders_lifetime', 'revenue_lifetime', 'categories_unique']:
                        errors.append(f"Feature '{feature_name}' cannot be negative: {value}")
        
        return errors
    
    def load_churn_model(self, model_path: Optional[str] = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ churn prediction –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—â–µ–º –≤ ml-engine/models)
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏
        """
        try:
            if model_path is None:
                model_path = self._find_churn_model()
            
            if not model_path or not os.path.exists(model_path):
                raise FileNotFoundError(f"Churn –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            
            logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ churn –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            self.churn_model = joblib.load(model_path)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏—é –∏ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏
            self.churn_model_version = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.churn_load_timestamp = datetime.now().isoformat()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º feature importance –∏–∑ –æ—Ç—á–µ—Ç–∞
            self._load_churn_feature_importance()
            
            logger.info("‚úÖ Churn –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            logger.info(f"   üìå –í–µ—Ä—Å–∏—è: {self.churn_model_version}")
            logger.info(f"   üïê –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {self.churn_load_timestamp}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ churn –º–æ–¥–µ–ª–∏: {e}")
            self._reset_churn_model()
            return False
    
    def _find_churn_model(self) -> Optional[str]:
        """–ü–æ–∏—Å–∫ churn –º–æ–¥–µ–ª–∏"""
        try:
            # –ò—â–µ–º –≤ ml-engine/models –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            base_path = Path(__file__).parent.parent.parent / "ml-engine" / "models"
            
            if not base_path.exists():
                logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_path}")
                return None
            
            # –ò—â–µ–º —Ñ–∞–π–ª churn_xgboost_model.pkl
            model_file = base_path / "churn_xgboost_model.pkl"
            
            if not model_file.exists():
                logger.warning("Churn –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return None
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–∞ churn –º–æ–¥–µ–ª—å: {model_file}")
            return str(model_file)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ churn –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def _load_churn_feature_importance(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ feature importance –∏–∑ –æ—Ç—á–µ—Ç–∞"""
        try:
            base_path = Path(__file__).parent.parent.parent / "ml-engine" / "models"
            report_file = base_path / "churn_model_report.txt"
            
            if report_file.exists():
                with open(report_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü–∞—Ä—Å–∏–º feature importance –∏–∑ –æ—Ç—á–µ—Ç–∞
                self.churn_feature_importance = {}
                lines = content.split('\n')
                for line in lines:
                    if ': 0.' in line and any(feature in line for feature in [
                        'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
                        'orders_lifetime', 'revenue_lifetime', 'categories_unique'
                    ]):
                        parts = line.split(':')
                        if len(parts) == 2:
                            feature = parts[0].strip()
                            importance = float(parts[1].strip())
                            self.churn_feature_importance[feature] = importance
                
                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–∞ feature importance: {len(self.churn_feature_importance)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –æ–±—É—á–µ–Ω–∏—è
                self.churn_feature_importance = {
                    'revenue_lifetime': 0.1806,
                    'aov_180d': 0.1648,
                    'monetary_180d': 0.1525,
                    'categories_unique': 0.1517,
                    'orders_lifetime': 0.1305,
                    'frequency_90d': 0.1168,
                    'recency_days': 0.1032
                }
                logger.info("üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è feature importance")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ feature importance: {e}")
            self.churn_feature_importance = {}
    
    def _reset_churn_model(self):
        """–°–±—Ä–æ—Å churn –º–æ–¥–µ–ª–∏"""
        self.churn_model = None
        self.churn_model_version = None
        self.churn_load_timestamp = None
        self.churn_feature_importance = None
    
    def is_churn_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ churn –º–æ–¥–µ–ª—å"""
        return self.churn_model is not None
    
    def predict_churn(self, features: Dict[str, Any]) -> Tuple[float, bool, List[str]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            features: –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–∞
            
        Returns:
            Tuple[float, bool, List[str]]: (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ, –ø—Ä–∏—á–∏–Ω—ã)
        """
        if not self.is_churn_model_loaded():
            raise RuntimeError("Churn –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            feature_df = pd.DataFrame([{
                'recency_days': features.get('recency_days', 999),  # 999 –µ—Å–ª–∏ –Ω–µ –ø–æ–∫—É–ø–∞–ª
                'frequency_90d': features.get('frequency_90d', 0),
                'monetary_180d': features.get('monetary_180d', 0),
                'aov_180d': features.get('aov_180d', 0),
                'orders_lifetime': features.get('orders_lifetime', 0),
                'revenue_lifetime': features.get('revenue_lifetime', 0),
                'categories_unique': features.get('categories_unique', 0)
            }])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            churn_probability = self.churn_model.predict_proba(feature_df)[0][1]
            will_churn = churn_probability >= 0.6  # threshold
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏—á–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ feature importance –∏ –∑–Ω–∞—á–µ–Ω–∏–π
            top_reasons = self._generate_churn_reasons(features, churn_probability)
            
            return churn_probability, will_churn, top_reasons
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞: {e}")
            raise
    
    def _generate_churn_reasons(self, features: Dict[str, Any], probability: float) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏—á–∏–Ω —Ä–∏—Å–∫–∞ –æ—Ç—Ç–æ–∫–∞"""
        reasons = []
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        thresholds = {
            'recency_days': 30,      # –±–æ–ª—å—à–µ 30 –¥–Ω–µ–π
            'frequency_90d': 1,      # –º–µ–Ω—å—à–µ 2 –∑–∞–∫–∞–∑–æ–≤
            'monetary_180d': 500,    # –º–µ–Ω—å—à–µ 500 —Ä—É–±–ª–µ–π
            'aov_180d': 200,         # –º–µ–Ω—å—à–µ 200 —Ä—É–±–ª–µ–π
            'orders_lifetime': 2,    # –º–µ–Ω—å—à–µ 3 –∑–∞–∫–∞–∑–æ–≤
            'revenue_lifetime': 1000, # –º–µ–Ω—å—à–µ 1000 —Ä—É–±–ª–µ–π
            'categories_unique': 1   # –º–µ–Ω—å—à–µ 2 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        if features.get('recency_days', 0) > thresholds['recency_days']:
            reasons.append("–¥–∞–≤–Ω–æ –Ω–µ –ø–æ–∫—É–ø–∞–ª")
        
        if features.get('frequency_90d', 0) < thresholds['frequency_90d']:
            reasons.append("–Ω–∏–∑–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫ –∑–∞ 90 –¥–Ω–µ–π")
        
        if features.get('monetary_180d', 0) < thresholds['monetary_180d']:
            reasons.append("–Ω–∏–∑–∫–∏–µ —Ç—Ä–∞—Ç—ã –∑–∞ 180 –¥–Ω–µ–π")
        
        if features.get('aov_180d', 0) < thresholds['aov_180d']:
            reasons.append("—Å–Ω–∏–∂–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞")
        
        if features.get('orders_lifetime', 0) < thresholds['orders_lifetime']:
            reasons.append("–º–∞–ª–æ –∑–∞–∫–∞–∑–æ–≤ –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è")
        
        if features.get('revenue_lifetime', 0) < thresholds['revenue_lifetime']:
            reasons.append("–Ω–∏–∑–∫–∞—è –æ–±—â–∞—è –≤—ã—Ä—É—á–∫–∞")
        
        if features.get('categories_unique', 0) < thresholds['categories_unique']:
            reasons.append("—É–∑–∫–∏–π –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç –ø–æ–∫—É–ø–æ–∫")
        
        # –ï—Å–ª–∏ –ø—Ä–∏—á–∏–Ω –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é
        if not reasons:
            if probability > 0.7:
                reasons.append("–≤—ã—Å–æ–∫–∏–π –æ–±—â–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞")
            elif probability > 0.5:
                reasons.append("—Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞")
            else:
                reasons.append("—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏—á–∏–Ω
        return reasons[:3]
    
    def validate_churn_features(self, features: Dict[str, Any]) -> List[str]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è churn prediction
        
        Args:
            features: –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        errors = []
        
        churn_feature_names = [
            'recency_days', 'frequency_90d', 'monetary_180d', 'aov_180d',
            'orders_lifetime', 'revenue_lifetime', 'categories_unique'
        ]
        
        for feature_name in churn_feature_names:
            if feature_name in features:
                value = features[feature_name]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–µ –∏–ª–∏ None
                if value is not None and not isinstance(value, (int, float)):
                    errors.append(f"Feature '{feature_name}' must be numeric, got {type(value).__name__}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if value is not None and value < 0:
                    errors.append(f"Feature '{feature_name}' cannot be negative: {value}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = ['frequency_90d', 'monetary_180d', 'orders_lifetime', 'revenue_lifetime', 'categories_unique']
        for field in required_fields:
            if field not in features or features[field] is None:
                errors.append(f"Required field '{field}' is missing or null")
        
        return errors


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
ml_service = MLModelService()
