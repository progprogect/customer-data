#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Train K-means model for all available dates and save results to database
Author: Customer Data Analytics Team
"""

import numpy as np
import pandas as pd
import psycopg2
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import logging
from typing import Tuple, Dict, List
import sys
import os
import json
from datetime import date, datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'dbname': 'customer_data',
    'user': 'mikitavalkunovich',
    'password': '',
    'port': 5432
}

def connect_to_db() -> psycopg2.extensions.connection:
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return conn
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def get_available_dates(conn: psycopg2.extensions.connection) -> List[date]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç –∏–∑ ml_user_features_daily_buyers"""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT DISTINCT snapshot_date 
            FROM ml_user_features_daily_buyers 
            ORDER BY snapshot_date
        """)
        dates = [row[0] for row in cur.fetchall()]
        cur.close()
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(dates)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return dates
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç: {e}")
        raise

def load_rfm_data_for_date(conn: psycopg2.extensions.connection, target_date: date) -> Tuple[pd.DataFrame, pd.Series]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ RFM –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–∞—Ç—ã"""
    sql = """
    SELECT
        user_id,
        recency_days,
        frequency_90d,
        monetary_180d,
        aov_180d,
        orders_lifetime,
        revenue_lifetime,
        categories_unique
    FROM ml_user_features_daily_buyers
    WHERE snapshot_date = %s
    ORDER BY user_id;
    """
    
    try:
        df = pd.read_sql(sql, conn, params=(target_date,))
        user_ids = df["user_id"]
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –¥–∞—Ç—ã {target_date}")
        return df, user_ids
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {target_date}: {e}")
        raise

def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, StandardScaler]:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = ["recency_days", "frequency_90d", "monetary_180d", "aov_180d", "categories_unique"]
    X = df[features].copy()
    
    # –õ–æ–≥-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–µ–Ω–µ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for col in ["monetary_180d", "aov_180d"]:
        X[col] = np.log1p(X[col])
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, scaler

def train_kmeans_model(X_scaled: np.ndarray, k: int = 3) -> Tuple[KMeans, np.ndarray]:
    """–û–±—É—á–µ–Ω–∏–µ K-means –º–æ–¥–µ–ª–∏"""
    kmeans = KMeans(
        n_clusters=k,
        random_state=42,
        n_init=10,
        max_iter=300
    )
    
    labels = kmeans.fit_predict(X_scaled)
    return kmeans, labels

def save_clustering_results(conn: psycopg2.extensions.connection, 
                          user_ids: pd.Series, 
                          cluster_labels: np.ndarray,
                          snapshot_date: date) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ë–î"""
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –¥–∞—Ç—É {snapshot_date}")
    
    try:
        cur = conn.cursor()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        data_to_insert = []
        for user_id, cluster_id in zip(user_ids, cluster_labels):
            data_to_insert.append((int(user_id), snapshot_date, int(cluster_id)))
        
        # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        insert_sql = """
        INSERT INTO user_segments_kmeans (user_id, snapshot_date, cluster_id)
        VALUES (%s, %s, %s)
        ON CONFLICT (user_id, snapshot_date) DO UPDATE SET
            cluster_id = EXCLUDED.cluster_id,
            created_at = NOW();
        """
        
        cur.executemany(insert_sql, data_to_insert)
        conn.commit()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        cur.execute("""
            SELECT cluster_id, COUNT(*) as count 
            FROM user_segments_kmeans 
            WHERE snapshot_date = %s 
            GROUP BY cluster_id 
            ORDER BY cluster_id
        """, (snapshot_date,))
        
        cluster_counts = cur.fetchall()
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {snapshot_date} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î:")
        for cluster_id, count in cluster_counts:
            logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        cur.close()
        
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è {snapshot_date}: {e}")
        raise

def create_segments_table(conn: psycopg2.extensions.connection) -> None:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã user_segments_kmeans"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS user_segments_kmeans (
        user_id BIGINT NOT NULL,
        snapshot_date DATE NOT NULL,
        cluster_id INT NOT NULL,
        created_at TIMESTAMPTZ DEFAULT NOW(),
        PRIMARY KEY (user_id, snapshot_date)
    );
    
    -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    CREATE INDEX IF NOT EXISTS idx_user_segments_user_id ON user_segments_kmeans(user_id);
    CREATE INDEX IF NOT EXISTS idx_user_segments_cluster ON user_segments_kmeans(cluster_id);
    CREATE INDEX IF NOT EXISTS idx_user_segments_date ON user_segments_kmeans(snapshot_date);
    
    -- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ —Ç–∞–±–ª–∏—Ü–µ
    COMMENT ON TABLE user_segments_kmeans IS '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π';
    COMMENT ON COLUMN user_segments_kmeans.user_id IS 'ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è';
    COMMENT ON COLUMN user_segments_kmeans.snapshot_date IS '–î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏';
    COMMENT ON COLUMN user_segments_kmeans.cluster_id IS 'ID –∫–ª–∞—Å—Ç–µ—Ä–∞ (0, 1, 2, ...)';
    """
    
    try:
        cur = conn.cursor()
        cur.execute(create_table_sql)
        conn.commit()
        cur.close()
        logger.info("–¢–∞–±–ª–∏—Ü–∞ user_segments_kmeans —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    except psycopg2.Error as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è K-means –¥–ª—è –≤—Å–µ—Ö –¥–∞—Ç"""
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è K-means –¥–ª—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    conn = connect_to_db()
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        create_segments_table(conn)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç
        available_dates = get_available_dates(conn)
        
        if not available_dates:
            logger.error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
            return
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(available_dates)} –¥–∞—Ç...")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã
        for i, target_date in enumerate(available_dates, 1):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã {i}/{len(available_dates)}: {target_date}")
            
            try:
                # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                df, user_ids = load_rfm_data_for_date(conn, target_date)
                
                if df.empty:
                    logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞—Ç—ã {target_date}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                X_scaled, scaler = preprocess_data(df)
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                kmeans, cluster_labels = train_kmeans_model(X_scaled, k=3)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                save_clustering_results(conn, user_ids, cluster_labels, target_date)
                
                logger.info(f"‚úÖ –î–∞—Ç–∞ {target_date} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞—Ç—ã {target_date}: {e}")
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        cur = conn.cursor()
        cur.execute("""
            SELECT snapshot_date, COUNT(*) as users_count 
            FROM user_segments_kmeans 
            GROUP BY snapshot_date 
            ORDER BY snapshot_date
        """)
        
        results = cur.fetchall()
        cur.close()
        
        logger.info("–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        for snapshot_date, users_count in results:
            logger.info(f"  {snapshot_date}: {users_count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ K-means –¥–ª—è –≤—Å–µ—Ö –¥–∞—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {e}")
        raise
    finally:
        if conn:
            conn.close()
            logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")

if __name__ == "__main__":
    main()

